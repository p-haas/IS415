{
  "hash": "433fa8b8aa8230e76b21250febe5b8ce",
  "result": {
    "markdown": "---\ntitle: \"Predicting HDB Public Housing Resale Prices using Geographically Weighted Methods\"\nsubtitle: \"Take-home Exercise 3\"\n\nauthor: \"Pierre HAAS\"\n\ndate: \"March 07, 2023\"\ndate-modified: \"2023-03-26\"\n\nexecute:\n  eval: true\n  echo: true\n  warning: false\neditor: visual\n\nnumber-sections: true\n---\n\n\n# Getting Started\n\nIn this Take-home Exercise 3, we will try using geographically weighted methods to predict the resale price of HDB public housing in Singapore. You will find in the next to sections a brief description of the data sets used and where to retrieve them as well as the list of all the libraries needed for this project and a description of their specific usage.\n\n## Installing and Loading Packages\n\nFor the purpose our this project, we have selected a list of libraries that will allow to perform all the necessary data cleaning, handling and analysis.\n\nThe following list shows the libraries that will be used and their purpose for this assignment:\n\n- pacman\n\n- tidyverse\n\n- janitor\n\n- sf\n\n- spdep\n\n- tmap\n\n- corrplot\n\n- olsrr\n\n- gtsummary\n\n- knitr\n\n- httr\n\n- onemapsapi\n\n- xml2\n\n- rvest\n\n- stringr\n\n- ggmap\n\n- ggpubr\n\n- GWmodel\n\n- SpatialML\n\n- Metrics\n\n- rsample\n\nTo install and load the packages, we use the p_load() function of the pacman package. It automatically checks if packages are installed, installs them if they are not installed, and loads the packages into the R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep,\n               GWmodel, tmap, tidyverse, gtsummary,\n               ranger, SpatialML, rsample, Metrics,\n               janitor)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(knitr, httr, onemapsgapi, xml2, rvest,\n               stringr, ggmap)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(digits = 15)\n```\n:::\n\n\n## The Data\n\nFor the purpose of this assignment, extensive amount of data was extracted from the web. You will find below a table that directs you to the webpage on which you can retrieve the data sets.\n\n# Retrieving and Importing the Geospatial Data\n\nIn this first section that addresses the data import, we will be going through the necessary work to load the geospatial data correctly into our R environment. \n\nNote that in this section, we will be load data previously retrieved from the web, but also using some APIs to collect necessary variables that will play an important role in our regression models.\n\n## Master Plan 2019 with sub-zones\n\n:::panel-tabset\n### Importing the data\n\nThe first geospatial data set that we are importing is the 2019 Master Plan with sub-zones. Since it is a shp file, we use the st_read() function of the sf package to properly import the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz.sf = st_read(dsn = \"data/geospatial/mpsz2019\",\n                  layer = \"MPSZ-2019\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `MPSZ-2019' from data source \n  `C:\\p-haas\\IS415\\Take-home_Ex\\Take-home_Ex03\\data\\geospatial\\mpsz2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.605700705134 ymin: 1.15869870063517 xmax: 104.088483065163 ymax: 1.47077483208461\nGeodetic CRS:  WGS 84\n```\n:::\n:::\n\n\nThe import was successful, however, before moving on I would like to check if all geometries are valid, and also if the projection code is correectly encoded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(st_is_valid(mpsz.sf) == FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nIt seems like we have invalid geometries. Now, let's check the CRS code of the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz.sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n```\n:::\n:::\n\n\nIt looks like the EPSG and coordinate system does not correspond to the Singaporean SVY21. So, we will change that. We pipe the st_transform() and st_make_valid() functions to solve the two issues we had just pointed out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz.sf = mpsz.sf %>%\n  st_make_valid() %>%\n  st_transform(3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nany(st_is_valid(mpsz.sf) == FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz.sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n```\n:::\n:::\n\n\nThe two issues seem to be solved. Note that I show you how to proceed when importing geospatial data here but will not do so in such an extensive manner later. My goal here is to show you how to replicate my way of proceeding when handling the data for the first time. To make the reading easier, I have cut out all my exploration of this webpage and left only the essential and necessary code.\n\n### Visualizing the data\n\nUsing the qtm() function of the tmap package, we can make a quick visualization of the geometry stored in the sf data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqtm(mpsz.sf)\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex03_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## MRT Train stations\n\nImporting MRT Exits into an sf data frame.\n\n:::panel-tabset\n### Importing the data\n\nAs explained just before, I will be sticking with the necessary code only from now on. Using the following code chunk, we import the MRT Train Stations into an sf data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt_exits.sf = st_read(dsn = \"data/geospatial/TrainStationExit/TrainStationExit\",\n                       layer = \"Train_Station_Exit_Layer\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `Train_Station_Exit_Layer' from data source \n  `C:\\p-haas\\IS415\\Take-home_Ex\\Take-home_Ex03\\data\\geospatial\\TrainStationExit\\TrainStationExit' \n  using driver `ESRI Shapefile'\nSimple feature collection with 562 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6134.08550000004 ymin: 27499.6967999991 xmax: 45356.3619999997 ymax: 47865.9227000009\nProjected CRS: SVY21\n```\n:::\n:::\n\n\n### Visualizing the data\n\nUsing the tmap package, we can conveniently plot the MRT Exits on the Singaporean map. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(mrt_exits.sf)+\n  tm_dots(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex03_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Bus Stop Locations\n\nImporting Bus Stops into an sf data frame.\n\n:::panel-tabset\n### Importing the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbus_stops.sf = st_read(dsn = \"data/geospatial/BusStopLocation/BusStop_Feb2023\",\n                       layer = \"BusStop\") %>%\n  st_transform(3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `BusStop' from data source \n  `C:\\p-haas\\IS415\\Take-home_Ex\\Take-home_Ex03\\data\\geospatial\\BusStopLocation\\BusStop_Feb2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5159 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.12161372974 ymin: 26482.1007183008 xmax: 48284.5630137296 ymax: 52983.8165183011\nProjected CRS: SVY21\n```\n:::\n:::\n\n\n### Visualizing the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(bus_stops.sf)+\n  tm_dots(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex03_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Supermarket data\n\nImporting Supermarket Locations into an sf data frame.\n\n:::panel-tabset\n### Importing the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsupermarkets.sf = st_read(\"data/geospatial/supermarkets/supermarkets-kml.kml\") %>%\n  st_transform(3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `SUPERMARKETS' from data source \n  `C:\\p-haas\\IS415\\Take-home_Ex\\Take-home_Ex03\\data\\geospatial\\supermarkets\\supermarkets-kml.kml' \n  using driver `KML'\nSimple feature collection with 526 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.625764663231 ymin: 1.2471504063107 xmax: 104.003578458884 ymax: 1.46152554782754\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n```\n:::\n:::\n\n\n### Visualizing the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(supermarkets.sf)+\n  tm_dots(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex03_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Retrieving the independent variables using the OneMap API\n\nWe will be using the One Map Singapore API to retrieve some of the independent variables that we will be using later on in our regression model.\n\nWe will be taking a few preliminary steps before extracting the variables. You will find everything you need to replicate my work in the following sections.\n\n### Retrieving the API token using the *onemapsgapi* package\n\nThe first step to getting the API key is to create an account on the One Map website (click [here](https://www.onemap.gov.sg/docs/#register-free-account) to access the registration tutorial).\n\nNow that you are registered, we may retrieve the API key using the *onemapsgapi* package and its get_token() function. By simply entering your registration email and password, you will get a valid API key for a three-day period.\n\nNote that if you want to learn more about the One Map API, you should definitely check their [website](https://www.onemap.gov.sg/docs/) and their [web app](https://app.swaggerhub.com/apis/onemap-sg/new-onemap-api/1.0.4) that allows to test the API's services.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoken_api = get_token(email = \"xxx@xxx.xxx\", \n                      password = \"...\")\n```\n:::\n\n\n### Using the API to retrieve the variables available on OneMap\n\nThe first step to retrieving our independent variables is to look at the available data provided by One Map. We do so using the httr package that allows us to make HTML queries. Using the GET() function, we retrieve every theme info of the OneMap API.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"https://developers.onemap.sg/privateapi/themesvc/getAllThemesInfo\"\n\nquery <- list(\"token\" = token_api)\nresult <- GET(url, query = query)\n```\n:::\n\n\nUsing the content() function, we may look at the information retrieved, however, you should be made aware that the result from our GET() command is quite messy. So, you will see in the next section how we extract the theme names and store them into a list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontent(result)\n```\n:::\n\n\n### Creating the variable list\n\nAs explained, the information retrieved is quite messy. So, we will be storing in a list the essential information that we will use later on.\n\nUsing a for loop, I store the theme specific information into a list called themes_list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthemes_list = list()\n\nfor (i in 1:length(content(result)$Theme_Names)){\n  themes_list = append(themes_list, content(result)$Theme_Names[[i]])\n}\n```\n:::\n\n\nTo take a look at the information stored into our list, you may use the next code chunk. However, since the list is quite long, I won't print the output/information here and will directly give you a list you the variables I have extracted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthemes_list\n```\n:::\n\n\nIn the next code chunk, you may see the variables that I carefully selected. They give information about: Eldercare Services, Hawker Centres, Food Courts, Parks in Singapore, Kindergartens, and Childcare Centres.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nind_variables = c(\"eldercare\", \"hawkercentre\", \"hawkercentre_new\", \"healthier_hawker_centres\", \"maxwell_fnb_map\", \"healthiercaterers\", \"relaxsg\", \"nationalparks\", \"kindergartens\", \"childcare\")\n```\n:::\n\n\nI have also picked-up some other interesting variables that may be included later in my regression model to improve the quality of the future model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextra_variables = c(\"lta_cycling_paths\", \"disability\", \"hsgb_fcc\", \"sportsg_sport_facilities\", \"ssc_sports_facilities\", \"wastedisposalsite\", \"drainageconstruction\", \"cpe_pei_premises\", \"sewerageconstruction\", \"danger_areas\", \"aquaticsg\", \"moh_isp_clinics\", \"heritagetrees\", \"nparks_activity_area\",  \"exercisefacilities\", \"mha_uav_2015\", \"playsg\", \"underconstruction_facilities\", \"preschools_location\", \"hdbcyclingunderconstruction\",  \"boundary_5km\",  \"hdbluppropunderconstruction\", \"parkingstandardszone\", \"libraries\", \"cyclingpath\", \"dengue_cluster\", \"greenbuilding\", \"nparks_uc_parks\", )\n```\n:::\n\n\nYou won't see these additional variables in my model due to the computing power of my computer. However, I strongly recommend to try an include these potential independent variables to improve the model.\n\n### Retrieving the available variables with the OneMap API\n\nUsing the code chunk below, you can retrieve the location name and its coordinates based on the previously created list of variables. We store this information into a data frame to make our use of this data easier\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame()\n\nurl = \"https://developers.onemap.sg/privateapi/themesvc/retrieveTheme\"\n\nfor (x in ind_variables){\n  \n  query <- list(\"queryName\" = x, \"token\" = token_api)\n  \n  result <- GET(url, query = query)\n  \n  print(\"Start\")\n  print(x)\n  \n  for (i in 2:length(content(result)$SrchResults)){\n    new_row = c(content(result)$SrchResults[[i]]$NAME,\n                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, \",\", 2)[1],\n                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, \",\", 2)[2],\n                content(result)$SrchResults[[1]]$Theme_Name)\n    \n    df = rbind(df, new_row)\n  }\n\n}\n\ncolnames(df)<-c(\"location_name\", \"lat\", \"lon\", \"variable_name\")\n```\n:::\n\n\nNote that retrieving the data takes a few minutes, so I took care of exporting the data frame into a csv file to avoid running the previous code chunk multiple times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(df, \"data/geospatial/retrieved_variables.csv\")\n```\n:::\n\n\nWe now have part of our independent variables stored into a csv file that we will import back into our R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nretrieved_variables = read_csv(\"data/geospatial/retrieved_variables.csv\", )\n```\n:::\n\n\nIf we look back to the variables we chose to retrieve using the One Map API, we had sub-categories to our independent variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(retrieved_variables$variable_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"eldercare\"      \"hawker_centres\" \"parks\"          \"kindergartens\" \n[5] \"child_care\"    \n```\n:::\n:::\n\n\nFor instance, we have four sub-categories for hawker centres and food courts. We shall simplify our work by renaming these sub-categories under one name only. Using the next code chunk, we may do so.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Eldercare Services\", \n                                                \"eldercare\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"^Hawker Centres$\", \n                                                \"hawker_centres\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Hawker Centres_New\", \n                                                \"hawker_centres\")\n\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Healthier Hawker Centres\", \n                                                \"hawker_centres\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Maxwell Chambers F&B map\", \n                                                \"hawker_centres\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Healthier Caterers\", \n                                                \"hawker_centres\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Parks@SG\", \n                                                \"parks\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Parks\", \n                                                \"parks\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Kindergartens\", \n                                                \"kindergartens\")\n\nretrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, \n                                                \"Child Care Services\", \n                                                \"child_care\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(retrieved_variables$variable_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"eldercare\"      \"hawker_centres\" \"parks\"          \"kindergartens\" \n[5] \"child_care\"    \n```\n:::\n:::\n\n\nWe shall now proceed to creating sf objects from the extracted coordinates.\n\n### Transforming the data frame to sf data frame\n\nUsing the st_as_sf() function from the *sf* package, we can transform our set of coordinates into sf point objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nretrieved_variables.sf = st_as_sf(retrieved_variables,\n                                  coords = c(\"lon\", \"lat\"),\n                                  crs = 4326, remove = FALSE) %>%\n  st_transform(3414)\n```\n:::\n\n\nWe shall check if our data points have been correctly transformed by plotting the points on the Singapore map using the *tmap* package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(retrieved_variables.sf)+\n  tm_dots()\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex03_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n# Retrieving and Importing the Aspatial Data\n\n## Resale Price of HDB flats\n\nAfter retrieving the resale HDB prices from the data.gov.sg website, I moved the folder into my data folder, it contains all the data sets necessary for our analysis, and more precisely stored it into the aspatial folder.\n\nThe downloaded data is stored in a folder called resale-flat-prices, itself containing multiple files. To choose the right file, we can use the base r function list.files() to return a list of the file's names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath = \"data/aspatial/resale-flat-prices\"\nfiles = list.files(path)\nfiles\n```\n:::\n\n\nLooking at the above list, it seems clear that we should use file n°5 as it contains the data from 2017 and later. It gives us the intuition that we may need to treat the data and take a subset of data rows within the analysis period (2021 to 2023). We will import the data using the read_csv() function from the readr package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_price = read_csv(file = paste(path, files[5], sep = \"/\"))\n```\n:::\n\n\nWe should take a quick look at the fields contained in the resale_price data frame. We do so using the glimpse() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(resale_price)\n```\n:::\n\n\nThe available fields are as follows:\n\n-   month: it indicates the month of transaction\n-   town: it indicates the district\n-   flat_type: it indicates the number of rooms in the flat\n-   block: it indicates the block number of the HDB flat\n-   street_name: it indicates the street number and name\n-   storey_range: it indicates the floor number of the flat\n-   floor_area_sqm: it indicates the floor area of the flat in square meter\n-   flat_model: it indicates the HDB flat model\n-   lease_commence_date: it indicates the starting year date of the lease (in Singapore, HDB flats are 99-year leaseholds)\n-   remaining_lease: it indicates the remaining years and months on the 99-year leasehold\n-   resale_price: it indicates the resale price of a given flat in SGD (Singapore dollars)\n\nNow, we will use the head() function to look at the available data in each field.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(resale_price)\n```\n:::\n\n\n### Selecting trasactions of 3 Room apartments from 2021 onwards\n\nMy previously mentioned intuition is confirmed. The month variable takes data starting in January 2017 and, through quick data manipulation, we will have to restrain the data frames to the 2021-2022 analysis period and 2023 data testing period. In addition, it looks like the town field in unnecessary and may be dropped. We only need the block and steet_name fields to geocode the addresses and create sf geometry.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms = resale_price %>%\n  select(-2) %>%\n  filter(grepl(\"202[123]\", month)) %>%\n  filter(flat_type == \"3 ROOM\")\n```\n:::\n\n\n### Transforming the storey_range column to dummy variables\n\nSince we would like to include the storey_range variable in our analysis and the column only takes categorical data, we shall create dummy variables to indicate the storey range of each HDB flat.\n\nThe first step is to look at what are the unique values in the column. Using the unique() function, we retrieve in the form of a list the unique categorical values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(rp_3rooms$storey_range)\n```\n:::\n\n\nNow, using a for loop and the ifelse() function, we will create a new column for each unique categorical variable contained in the storey_range field and assign a 1 if the particular HDB flat belongs to the specific storey range.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in unique(rp_3rooms$storey_range)){\n  rp_3rooms[i] = ifelse(rp_3rooms$storey_range == i, 1, 0)\n}\n```\n:::\n\n\n### Transforming the flat_model column to dummy variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(rp_3rooms$flat_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in unique(rp_3rooms$flat_model)){\n  rp_3rooms[i] = ifelse(rp_3rooms$flat_model == i, 1, 0)\n}\n```\n:::\n\n\n### Transforming the remaining_lease to a numerical variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlease_remaining = list()\n\nfor (i in 1:nrow(rp_3rooms)){\n  \n  lease = str_extract_all(rp_3rooms$remaining_lease, \"[0-9]+\")[[i]]\n  \n  year = as.numeric(lease[1])\n  \n  if (length(lease) < 2){\n    \n    lease_remaining = append(lease_remaining, year)\n  \n    } else {\n      \n    month = as.numeric(lease[2])\n    \n    lease_remaining = append(lease_remaining, round(year+month/12, 2))\n  \n    }\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms$remaining_lease = as.numeric(lease_remaining)\n```\n:::\n\n\n### Geocoding the address\n\nThe first step to geocoding the address - retrieving the latitude and longitude based in the address - is to create a new column that stores the cleaned full address. Consequently, we create a new field called cleaned_address that combines both the block and street_name fields. This will allow us to retrieve the geocode of the HDB flats in our query.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms[\"cleaned_address\"] = paste(rp_3rooms$block, rp_3rooms$street_name, sep = \" \")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"https://developers.onemap.sg/commonapi/search\"\n\nfull_address = list()\nlatitude = list()\nlongitude = list()\n\nfor (address in rp_3rooms$cleaned_address){\n  query <- list(\"searchVal\" = address,\n                \"returnGeom\" = \"Y\", \"getAddrDetails\" = \"Y\")\n  res <- GET(url, query = query)\n  \n  full_address = append(full_address, content(res)$results[[1]]$ADDRESS)\n  latitude = append(latitude, content(res)$results[[1]]$LATITUDE)\n  longitude = append(longitude, content(res)$results[[1]]$LONGITUDE)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms$full_address = full_address\nrp_3rooms$lat = latitude\nrp_3rooms$lon = longitude\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(digits=15)\n\nrp_3rooms$full_address = apply(rp_3rooms[, 28], 2, as.character)\nrp_3rooms$lat = apply(rp_3rooms[, 29], 2, as.numeric)\nrp_3rooms$lon = apply(rp_3rooms[, 30], 2, as.numeric)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(rp_3rooms, \"data/aspatial/geocoded_resale_price.csv\", row.names = FALSE)\n```\n:::\n\n\nWe import the geocoded data back into the R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms = read_csv(\"data/aspatial/geocoded_resale_price.csv\")\n```\n:::\n\n\n### Dropping the unrelevant variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.cleaned = rp_3rooms %>%\n  select(-c(\"flat_type\", \"block\", \"street_name\", \"storey_range\", \"flat_model\", \"cleaned_address\", \"full_address\"))\n```\n:::\n\n\n### Creating the sf geometry\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf = st_as_sf(rp_3rooms.cleaned,\n                        coords = c(\"lon\", \"lat\"),\n                        crs = 4326, remove = FALSE) %>%\n  st_transform(3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(rp_3rooms.sf)+\n  tm_dots()\n```\n:::\n\n\n## Primary School data\n\n### Scrapping Primary Schools online with *rvest* package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"https://sgschooling.com/school/\"\n\nres = GET(url)\n\nwiki_read <- xml2::read_html(res, encoding = \"UTF-8\")\n\ntestt = wiki_read %>%\n  html_elements(\"a\")\n\ntestt1 = testt[grepl(\"school/\", testt)]\n\npattern <- \">(.*)<\"\nresult <- regmatches(testt1, regexec(pattern, testt1))\n\nschools = list()\n\nfor (i in 1:length(result)){\n  new_row = result[[i]][2]\n  schools = append(schools, new_row)\n}\n```\n:::\n\n\n### Using the OneMap API to retrieve the schools' coordinates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc = data.frame()\n\nurl = \"https://developers.onemap.sg/commonapi/search\"\n\nfor (i in 1:length(schools)){\n  \n  query <- list(\"searchVal\" = schools[[i]],\n                \"returnGeom\" = \"Y\", \n                \"getAddrDetails\" = \"Y\")\n  result <- GET(url, query = query)\n  \n  if (length(content(result)$results) == 0){\n    new_row = c(schools[[i]], \n              NA,\n              NA,\n              NA)\n    primary_sc = rbind(primary_sc, new_row)\n    \n  } else{\n    new_row = c(schools[[i]], \n                content(result)$results[[1]]$ADDRESS,\n                content(result)$results[[1]]$LATITUDE,\n                content(result)$results[[1]]$LONGITUDE)\n    \n    primary_sc = rbind(primary_sc, new_row)\n    \n  }\n\n}\n\ncolnames(primary_sc) = c(\"location_name\", \"full_address\", \"lat\", \"lon\")\n```\n:::\n\n\n### Checking for missing information\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(primary_sc$full_address))\n```\n:::\n\n\nIt seems like we have missing data for 9 primary schools. It is probably because the name of the school is not recognized by the API, so we will try to clean them to get their information into our data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc %>%\n  filter(is.na(primary_sc$full_address) == TRUE) %>%\n  select(1)\n```\n:::\n\n\nBy checking on the web, it seems like the \"Juying Primary School\" has merged with the \"Pioneer Primary School\", so we may drop the \"Juying Primary School\" from the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc1 = primary_sc %>%\n  filter(!grepl(\"Juying Primary School\", location_name))\n```\n:::\n\n\nNow, regarding the remaining schools, after performing a tests, it seems like I am not able to retrieve the information using the API so we will be filling the values with, first, the Google Maps API and then, if there are still some empty information, I will be filling the values manually by looking online.\n\nYou may want to check the next two code chunks I have used to try and retrieve the missing information using the OneMap API.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc1$location_name = str_replace(primary_sc1$location_name, \n                                        \"St. \", \"\")\n\n# I have also tried replacing the string with: \"Saint\" and \"St\" but I am not able to make the API retrieve the data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"https://developers.onemap.sg/commonapi/search\"\n\nfor (i in 1:nrow(primary_sc1)){\n  \n  if (is.na(primary_sc1[i, 2])){\n    query <- list(\"searchVal\" = primary_sc1[i, 1],\n                  \"returnGeom\" = \"Y\", \n                  \"getAddrDetails\" = \"Y\")\n    result <- GET(url, query = query)\n    \n    if (length(content(result)$results) == 0){\n      next\n      \n      } else{\n        primary_sc1[i, 2] = content(result)$results[[1]]$ADDRESS\n        primary_sc1[i, 3] = content(result)$results[[1]]$LATITUDE\n        primary_sc1[i, 4] = content(result)$results[[1]]$LONGITUDE\n    \n  }} else{ next\n  \n  }\n}\n```\n:::\n\n\nThese are the schools with the missing information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc1 %>%\n  filter(is.na(primary_sc1$full_address) == TRUE) %>%\n  select(1)\n```\n:::\n\n\nUsing the Google Maps API and its library *ggmap*, we will loop through the schools with missing coordinates. Since all these schools are located consecutively in the data frame, I use the following structure to create the list of schools.\n\nWhile looping through these schools, we use the geocode() function of *ggmap* to retrieve the latitude and longitude based on the school name and assign these values in the corresponding missing cells of the primary_sc1 data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregister_google(key = \"xxx\")\n\nmissing_sc = primary_sc1[grep(\"St. Andrew’s Junior School\", primary_sc1$location_name):grep(\"St. Stephen’s School\", primary_sc1$location_name), 1]\n\nfor (i in missing_sc){\n  coords = geocode(i)\n  primary_sc1[grep(i, primary_sc1$location_name), 3] = coords$lat\n  primary_sc1[grep(i, primary_sc1$location_name), 4] = coords$lon\n  \n}\n```\n:::\n\n\nWe should check if we still have missing coordinates. Note that we are missing information in the full_address column, however, it is not a great deal since we will be using the coordinates to create the sf objects later on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc1 %>%\n  filter(is.na(primary_sc1$lat) == TRUE)\n```\n:::\n\n\nWe are still missing coordinates for three schools. We will be filling these values manually.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_sc1[grep(\"St. Anthony’s Primary School\", primary_sc1$location_name), 3:4] = list(1.3796337949311097, 103.75033229288428)\n\nprimary_sc1[grep(\"St. Gabriel’s Primary School\", primary_sc1$location_name), 3:4] = list(1.3499742816603595, 103.86268328706147)\n\nprimary_sc1[grep(\"St. Stephen’s School\", primary_sc1$location_name), 3:4] = list(1.3189794629543288, 103.9179118196036)\n```\n:::\n\n\nWe are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(primary_sc1, \"data/aspatial/primary_schools.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_schools = read_csv(\"data/aspatial/primary_schools.csv\")\n```\n:::\n\n\n### Creating the sf data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprimary_schools.sf = st_as_sf(primary_schools,\n                         coords = c(\"lon\", \"lat\"),\n                         crs = 4326, remove = FALSE) %>%\n  st_transform(3414)\n```\n:::\n\n\n## Shopping Mall data\n\n### Scraping Wikipedia to extract the list of malls in Singapore\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore\"\n\nres = GET(url)\n\nwiki_read <- xml2::read_html(res, encoding = \"UTF-8\")\n\ntestt = wiki_read %>%\n  html_nodes(\"div.div-col\") %>%\n  html_elements(\"ul\") %>%\n  html_elements(\"li\") %>%\n  html_text()\n\nmalls = str_replace(testt, \"\\\\[.*]\", \"\")\n```\n:::\n\n\n### Using the OneMap API to retrieve the malls' coordinates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls = data.frame()\n\nurl = \"https://developers.onemap.sg/commonapi/search\"\n\nfor (i in 1:length(malls)){\n  \n  query <- list(\"searchVal\" = malls[i],\n                \"returnGeom\" = \"Y\", \n                \"getAddrDetails\" = \"Y\")\n  result <- GET(url, query = query)\n  \n  if (length(content(result)$results) == 0){\n    new_row = c(malls[i], \n              NA,\n              NA,\n              NA)\n    shp_malls = rbind(shp_malls, new_row)\n    \n  } else{\n    new_row = c(malls[i], \n                content(result)$results[[1]]$ADDRESS,\n                content(result)$results[[1]]$LATITUDE,\n                content(result)$results[[1]]$LONGITUDE)\n    \n    shp_malls = rbind(shp_malls, new_row)\n    \n  }\n\n}\n\ncolnames(shp_malls) = c(\"location_name\", \"full_address\", \"lat\", \"lon\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(shp_malls$full_address))\n```\n:::\n\n\n### Filling missing information with ggmap\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls %>%\n  filter(is.na(shp_malls$full_address) == TRUE) %>%\n  select(1)\n```\n:::\n\n\nBefore using the Google Maps API, I decided to research about these nine malls and found that:\n\n-   The PoMo mall is now called GR.iD, so we shall change the name in the data frame\n\n-   The KINEX mall is found under the name KINEX, so we shall remove the additional information from the name\n\n-   The Paya Lebar Quarter mall is found under the name PLQ mall on Google Maps, so we shall change the name in the data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls[grep(\"PoMo\", shp_malls$location_name), 1] = \"GR.iD\";\n\nshp_malls[grep(\"KINEX\", shp_malls$location_name), 1] = \"KINEX\";\n\nshp_malls[grep(\"PLQ\", shp_malls$location_name), 1] = \"PLQ mall\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nregister_google(key = \"xxx\")\n\nmissing_malls = shp_malls %>%\n  filter(is.na(shp_malls$full_address) == TRUE) %>%\n  `$`(location_name)\n\nfor (i in list(missing_malls)[[1]]){\n  coords = geocode(i)\n  shp_malls[grep(i, shp_malls$location_name), 3] = coords$lat\n  shp_malls[grep(i, shp_malls$location_name), 4] = coords$lon\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls %>%\n  filter(is.na(shp_malls$lat) == TRUE)\n```\n:::\n\n\n### Filling missing information manually\n\nWe are still missing coordinates of the KINEX mall. We will be filling these values manually.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls[grep(\"KINEX\", shp_malls$location_name), 3:4] = c(1.314893715213727, 103.89480904154526)\n```\n:::\n\n\nWe are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(shp_malls, \"data/aspatial/shopping_malls.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls = read_csv(\"data/aspatial/shopping_malls.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshp_malls.sf = st_as_sf(shp_malls,\n                        coords = c(\"lon\", \"lat\"),\n                        crs = 4326, remove = FALSE) %>%\n  st_transform(3414)\n```\n:::\n\n\n# Preparing the final data frame\n\nIn this section, we will prepare the following variables:\n\nProxomity to CBD Proximity to eldercare Proximity to foodcourt/hawker centres Proximity to MRT Proximity to park Proximity to good primary school Proximity to shopping mall Proximity to supermarket Numbers of kindergartens within 350m Numbers of childcare centres within 350m Numbers of bus stop within 350m Numbers of primary school within 1km\n\n## Proximity variables\n\n### Proximity to eldercare\n\n\n::: {.cell}\n\n```{.r .cell-code}\neldercare.sf = retrieved_variables.sf %>%\n  filter(variable_name == \"eldercare\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, eldercare.sf)\n\nrp_3rooms.sf$distance_to_eldercare = as.numeric(\n  st_distance(rp_3rooms.sf, \n              eldercare.sf[nearest,],\n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to food court / hawker centres\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhawker_centres.sf = retrieved_variables.sf %>%\n  filter(variable_name == \"hawker_centres\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, hawker_centres.sf)\n\nrp_3rooms.sf$distance_to_food = as.numeric(\n  st_distance(rp_3rooms.sf, \n              hawker_centres.sf[nearest,], \n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to MRT\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, mrt_exits.sf)\n\nrp_3rooms.sf$distance_to_mrt = as.numeric(\n  st_distance(rp_3rooms.sf, \n              mrt_exits.sf[nearest,],\n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to park\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparks.sf = retrieved_variables.sf %>%\n  filter(variable_name == \"parks\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, parks.sf)\n\nrp_3rooms.sf$distance_to_park = as.numeric(\n  st_distance(rp_3rooms.sf, \n              parks.sf[nearest,],\n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to good primary school\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, shp_malls.sf)\n\nrp_3rooms.sf$distance_to_mall = as.numeric(\n  st_distance(rp_3rooms.sf, \n              shp_malls.sf[nearest,],\n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to shopping mall\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, shp_malls.sf)\n\nrp_3rooms.sf$distance_to_mall = as.numeric(\n  st_distance(rp_3rooms.sf, \n              shp_malls.sf[nearest,], \n              by_element=TRUE) )\n```\n:::\n\n\n### Proximity to supermarket\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnearest = st_nearest_feature(rp_3rooms.sf, supermarkets.sf)\n\nrp_3rooms.sf$distance_to_supermarkets = as.numeric(\n  st_distance(rp_3rooms.sf, \n              supermarkets.sf[nearest,], \n              by_element=TRUE) )\n```\n:::\n\n\n## Number of .. within distance variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuffer_350  = st_buffer(rp_3rooms.sf, 350)\nbuffer_1000 = st_buffer(rp_3rooms.sf, 1000)\n```\n:::\n\n\n### Numbers of kindergartens within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkindergartens.sf = retrieved_variables.sf %>%\n  filter(variable_name == \"kindergartens\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf$nb_of_kindergartens = lengths(\n  st_intersects(buffer_350, kindergartens.sf))\n```\n:::\n\n\n### Numbers of childcare centres within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchild_care.sf = retrieved_variables.sf %>%\n  filter(variable_name == \"child_care\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf$nb_of_childcare = lengths(\n  st_intersects(buffer_350, child_care.sf))\n```\n:::\n\n\n### Numbers of bus stop within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf$nb_of_bus_stops = lengths(\n  st_intersects(buffer_350, bus_stops.sf))\n```\n:::\n\n\n### Numbers of primary school within 1km\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf$nb_of_primary_schools = lengths(\n  st_intersects(buffer_1000, primary_schools.sf))\n```\n:::\n\n\n## Saving the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(rp_3rooms.sf, \"data/geospatial/final_dataset.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf = read_rds(\"data/geospatial/final_dataset.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(rp_3rooms.sf))\n```\n:::\n\n\n# Hedonic Pricing model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf = rp_3rooms.sf %>%\n  rename(\"01_to_03\" = \"01 TO 03\", \"04_to_06\" = \"04 TO 06\", \"07_to_09\" = \"07 TO 09\",\n         \"10_to_12\" = \"10 TO 12\", \"13_to_15\" = \"13 TO 15\", \"16_to_18\" = \"16 TO 18\",\n         \"19_to_21\" = \"19 TO 21\", \"22_to_24\" = \"22 TO 24\", \"25_to_27\" = \"25 TO 27\",\n         \"28_to_30\" = \"28 TO 30\", \"31_to_33\" = \"31 TO 33\", \"34_to_36\" = \"34 TO 36\",\n         \"37_to_39\" = \"37 TO 39\", \"40_to_42\" = \"40 TO 42\", \"43_to_45\" = \"43 TO 45\",\n         \"46_to_48\" = \"46 TO 48\", \"New_Generation\" = \"New Generation\", \n         \"Model_A\" = \"Model A\", \"Premium_Apartment\" = \"Premium Apartment\")\n```\n:::\n\n\n## Splitting the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_analysis.sf = rp_3rooms.sf %>%\n  filter(grepl(\"202[12]\", month))\n\nrp_testing.sf = rp_3rooms.sf %>%\n  filter(grepl(\"2023\", month))\n```\n:::\n\n\n## Conventional OLS method\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(rp_3rooms.sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_analysis.mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease +\n                      `01_to_03` + `04_to_06` + `07_to_09` + `10_to_12` + `13_to_15` + \n                      `16_to_18` + `19_to_21` + `22_to_24` + `25_to_27` + `28_to_30` + \n                      `31_to_33` + `34_to_36` + `37_to_39` + `40_to_42` + `43_to_45` + \n                      `New_Generation` + Improved + `Model_A` + Simplified + Standard + \n                      `Premium_Apartment` + DBSS + distance_to_eldercare + \n                      distance_to_food + distance_to_mrt + distance_to_park + \n                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +\n                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, \n                    data = rp_analysis.sf)\n\nsummary(rp_analysis.mlr)\n```\n:::\n\n\n## GWR methods\n\n### Converting sf data frame to SpatialPointDataFrame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_analysis.sp = as_Spatial(rp_analysis.sf)\n```\n:::\n\n\n### Computing adaptive bandwidth\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw_analysis_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + remaining_lease +\n                      X01_to_03 + X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + \n                      X16_to_18 + X19_to_21 + X22_to_24 + X25_to_27 + X28_to_30 + \n                      X31_to_33 + X34_to_36 + X37_to_39 + X40_to_42 + X43_to_45 + \n                      New_Generation + Improved + Model_A + Simplified + Standard + \n                      Premium_Apartment + DBSS + distance_to_eldercare + \n                      distance_to_food + distance_to_mrt + distance_to_park + \n                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +\n                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, \n                      data = rp_analysis.sp, approach=\"CV\", kernel=\"gaussian\",\n                      adaptive=TRUE, longlat=FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(bw_analysis_adaptive, \"data/model/bw_analysis_adaptive.rds\")\n```\n:::\n\n\n### Constructing the adaptive bandwidth gwr model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw_adaptive <- read_rds(\"data/model/bw_analysis_adaptive.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + remaining_lease + X01_to_03 +\n                            X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + X16_to_18 + X19_to_21 +\n                            X22_to_24 + X25_to_27 + X28_to_30 + X31_to_33 + X34_to_36 + X37_to_39 + \n                            X40_to_42 + X43_to_45 + New_Generation + Improved + Model_A + Simplified + \n                            Standard + Premium_Apartment + DBSS + distance_to_eldercare + distance_to_food + \n                            distance_to_mrt + distance_to_park + distance_to_mall + distance_to_supermarkets + \n                            nb_of_kindergartens + nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, \n                          data = rp_analysis.sp, bw=bw_adaptive, kernel = 'gaussian', adaptive=TRUE, \n                          longlat = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(gwr_adaptive, \"data/model/gwr_analysis_adaptive.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive = read_rds(\"data/model/gwr_analysis_adaptive.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive\n```\n:::\n\n\n## Dropping geometry\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train <- st_coordinates(rp_analysis.sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(coords_train, \"data/model/coords_train.rds\" )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train = read_rds(\"data/model/coords_train.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_analysis.no_geom = rp_analysis.sf %>% \n  st_drop_geometry() %>%\n  select(-c(1, 3, 22:23))\n```\n:::\n\n\n## Calibrating Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_analysis.no_geom = rp_analysis.no_geom %>%\n  clean_names()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2410)\nrf <- ranger(resale_price ~ floor_area_sqm + remaining_lease +\n               x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + \n               x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + \n               x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + \n               new_generation + improved + model_a + simplified + standard + \n               premium_apartment + dbss + distance_to_eldercare + \n               distance_to_food + distance_to_mrt + distance_to_park + \n               distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +\n               nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, \n             data = rp_analysis.no_geom)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(rf)\n```\n:::\n\n\n## Calibrating Geographical Random Forest Model\n\n### Calibrating using training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + remaining_lease +\n                       x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + \n                       x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + \n                       x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + \n                       new_generation + improved + model_a + simplified + standard + \n                       premium_apartment + dbss + distance_to_eldercare + \n                       distance_to_food + distance_to_mrt + distance_to_park +\n                       distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +\n                       nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools,\n                     dframe=rp_analysis.no_geom, bw=bw_adaptive, kernel=\"adaptive\",\n                     coords=coords_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive250.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_adaptive <- read_rds(\"data/model/gwRF_adaptive.rds\")\n```\n:::\n\n\n# Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(mpsz.sf)+\n  tm_polygons()+\ntm_shape(rp_3rooms.sf)+\n  tm_dots(col = \"blue\")+\ntm_shape(st_buffer(rp_3rooms.sf, 350))+\n  tm_fill(col = \"red\",\n          alpha = 0.1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_3rooms.sf %>%\n  filter(st_distance(rp_3rooms.sf, kindergartens.sf) <= 350)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspatialrisk::concentration(rp_3rooms.sf, kindergartens.sf,\n                           value = count, radius = 350)\n```\n:::\n",
    "supporting": [
      "Take-home_Ex03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}