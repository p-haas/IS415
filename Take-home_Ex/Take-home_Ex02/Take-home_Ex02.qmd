---
title: "Take-home Exercise - Application of Spatial Point Patterns Analysis"
subtitle: "Discover the geographical distribution of functional and non-function water points in Osun State, Nigeria"

author: "Pierre HAAS"

date: "February 18, 2023"
date-modified: "`r Sys.Date()`"

execute:
  eval: true
  echo: true
  warning: false
editor: visual

number-sections: true
---

# Getting started

## Installing and Loading Packages

```{r}
pacman::p_load(sf, sfdep, tmap, plyr, tidyverse, readxl, magrittr, knitr, plotly, Kendall, tidyr)
```

## Retrieve the data from the web



# Importing data

We will import the data in two steps, first the geospatial data and then the aspatial data.

## Geospatial data

We import the geospatial data using the st_read() function of the sf package.

```{r}
dki = st_read(dsn = "data/geospatial",
             layer = "BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA")
```
This sf data frame contains Multipolygon geometry type. The data seems to be encoded in WGS 84, we will need to perform a transformation the projected system of Indonesia and Jakarta.

### Data exploration

Let's take a peak into the data frame using the glimpse() function.

```{r}
glimpse(dki)
```

Looking at the above fields, it seems that we are only interested in the first nine columns. We will be selecting them using the select() function.

Note that the geometry field will remain even if not selected.

```{r}
dki = dki %>%
  select(c(1:9)) %>%
  rename("GeoCode" = 2)
```

For the purpose of this hands-on exercise and easier processes later, I take the freedom to rename the second column

#### Transform CRS

```{r}
#| eval: false
dki %>%
  st_crs()
```

```{r}
dki = dki %>%
  st_transform(crs=23837)
```

#### Check NA values and duplicates

```{r}
#| eval: false
any(is.na(dki))
```

```{r}
#| eval: false
head(dki[is.na(dki$DESA_KELUR),])
```

The Kelurahan information seems to be missing. We will be using the DESA and KODE_DESA to cross check with the aspatial data what may be the name to fill in the rows. We could consider dropping these two rows now, but I prefer to wait and confirm before dropping NA values.

## Aspatial data

Since we need to merge a series of data sets, we will first take a quick look a them to understand what are the relevant fields for our analysis and what work needs to be performed to have one single data set that will allow us to find the monthly vaccination rate.

The first step is to look at the data set of June 2021. Even though this data set takes record of data from the month prior our analysis period (July 2021 - June 2022) and does not seem relevant, I still think that it is important to compute the monthly vaccination (in dosis administered) and consequently we start with the data of June 2021.

```{r}
vaccination = read_excel("data/aspatial/before.xlsx")
glimpse(vaccination)
```

Using the glimpse() function we can take a look at the different fields of the data sets that will be imported next. Looking at the fields, it looks like we will be selecting only 3 fields:

-   *KELURAHAN*; this field gives information about the sub-district

-   *SASARAN*; this field represents the vaccination target of each sub-district

-   *BELUM VAKSIN*; this field informs about the population that is yet-to-be vaccinated

Before moving on with the examination of the aspatial data, we shall quickly check for the missing values found in the previous data frame.

```{r}
#| eval: false
vaccination %>%
  filter(`KODE KELURAHAN` == 31888888)
```

We may want to check based on the name and will be looking for cells that may contain the DANAU string.

```{r}
#| eval: false
vaccination %>%
  filter(grepl('Danau', KELURAHAN))
```

It seems like we can drop the two rows from the dki data set, however, for the purpose of our analysis, I prefer to keep these two sub-districts in the data frame. We may have missing values in our choropleth maps, but it is no problem.

Please find the code chunk necessary to drop NA values in case you deem it to be necessary.

```{r}
#| eval: false
dki = na.omit(dki)
```

```{r}
vaccination = vaccination %>%
  select(1, 4, 5, 6) %>%
  rename("GeoCode" = 1, "Target" = 3, "Yet-to-be Vaccinated" = 4)

vaccination$Date = as.Date("2021-06-30")
```

```{r}
str(vaccination)
```

Now that we have taken a look at the data and selected the relevant fields, we can move on with the data cleaning and merge the vaccination data from July 2021 to June 2022 into the vaccination data frame.

To do so, we will be using a for loop in which we will be performing the following tasks:

-   Create a list called months that stores the name of every month in our analysis period, this will allow us to customize the name of the columns;

-   Line 1 and 2 of the loop help us count the month number of our analysis. If you take a quick peak into the later data sets, it seems like Indonesia and the Jakarta region adopted a 3-dosis vaccination plan, meaning that the *TOTAL VAKSIN DIBERIKAN* is now located on the 10th column from March 2022 on-wards;

-   Using the *paste()* function, we replicate the name of the data sets stored in our data folder and use the *read_excel()* function of the **readxl** package, we store the data frame into the variable df;

-   The remaining part of the code serves the purpose of storing Vaccination target data and Yet-to-be Vaccinated data;

```{r}
#| warning: false

records = seq(from = as.Date("2021-08-01"), to = as.Date("2022-07-01"), by = 'months') - 1

months = c('July', 'August', 'September', 'October', 
           'November', 'December', 'January', 'February', 
           'March', 'April', 'May', 'June')

count = 1

for (i in months){
  filename = paste("data/aspatial/", i, ".xlsx", sep = "")
  
  df = read_excel(filename) %>% 
    select(1, 4,5,6) %>%
    rename("GeoCode" = 1, "Target" = 3, "Yet-to-be Vaccinated" = 4)
  
  df$Date = records[count]
  
  vaccination = rbind(vaccination, df)
  
  count = count + 1
}
```

Before merging the aspatial data -- vaccination data frame -- with the geospatial data -- dki data frame -- we will perform some data exploration to remove any redundant fields and look for any potential duplicates or missing values.

### Data exploration

We will begin our data exploration by looking at the target columns. I have an intuition that target data might be the same across some periods of our analysis. To do so, we select only the columns that contain the string Target using the *grepl()* function.

```{r}
#| eval: false
head(vaccination[ , grepl( "Target" , names( vaccination ) ) ])
```

Looking at the above print, it looks like target data is the same across our 12-month analysis period, thus we will drop the all the above columns and keep only one column for our reference.

```{r}
vaccination[grep('TOTAL', vaccination$KELURAHAN),]
```

It looks like the only vaccination target that differs is the one prior to our analysis period. We won't drop it yet but will drop it as soon as we pivot our data frame.

```{r}
#| eval: false
head(vaccination)
```

```{r}
#| eval: false
kable(head(vaccination))
```

Now, we will look for any potential NA values. We shall use the code chunk below to check for them.

```{r}
#| eval: false
any(is.na(vaccination))
```

There are no missing values in the data frame, we will now pivot the data frame.

### Creating a pivot table

```{r}
vaccination_pivot = vaccination %>%
  pivot_wider(names_from = Date, values_from = c(`Yet-to-be Vaccinated`, Target))
```

```{r}
head(vaccination_pivot[ , grepl( "Target" , names( vaccination_pivot ) ) ])
```

Looking at the above print, it looks like target data is the same across our 12-month analysis period, thus we will drop the all the above columns and keep only one column for our reference.

```{r}
vac_target = vaccination_pivot[ , grepl("Target_2021-07-31", names(vaccination_pivot))] %>%
  setNames(c("Vaccination_Target"))

vaccination_pivot = cbind(vaccination_pivot[ , !grepl( "Target" , names( vaccination_pivot ) ) ], vac_target)
```

```{r}
head(vaccination_pivot)
```

Looking at the above print, we have successfully replaced all the target columns for a single one that is now located at the right end of the data frame. We will push it to the front with the following code chunk.

```{r}
vaccination_pivot = vaccination_pivot %>% 
  relocate("Vaccination_Target", .after="KELURAHAN")
```

# Merging the data

```{r}
dataMerged = merge(dki, vaccination_pivot, 
             by.x = "DESA_KELUR", by.y = "KELURAHAN", 
             all = TRUE)
```

```{r}
#| eval: false
head(dataMerged)
```

```{r}
#| eval: false
any(duplicated(dataMerged))
```

```{r}
#| eval: false
any(is.na(dataMerged))
```

It looks like there are NA values even though both data frames were properly cleaned. We should take a peak at the data to understand where this issue comes from.

```{r}
#| eval: false
glimpse(dataMerged[is.na(dataMerged$`Vaccination Target`),])
```

```{r}
#| eval: false
glimpse(dataMerged[st_is_empty(dataMerged),])
```

```{r}
list.a = setdiff(dki$DESA_KELUR, vaccination$KELURAHAN)
list.a = list.a[order(list.a)]
```

```{r}
#| eval: false
list.b = setdiff(vaccination$KELURAHAN, dki$DESA_KELUR)
list.b
```

We should take the "TOTAL" value out both of the above list and data frame.

```{r}
#| eval: false
list.b = list.b[-1]
```

```{r}
vaccination_pivot = vaccination_pivot %>%
  filter(!grepl('TOTAL', KELURAHAN))
```

```{r}
#| eval: false
list.b[order(list.b)]
```

```{r}
count = 0
region = c("BALE KAMBANG", "HALIM PERDANA KUSUMAH", "JATI PULO", 
           "KRAMAT JATI", "KERENDANG", "PAL MERIAM", "PINANG RANTI", 
           "RAWA JATI", "KAMPUNG TENGAH")

for (i in list.a){
  count = count + 1
  dki$DESA_KELUR[dki$DESA_KELUR == i] <- region[count]
}
```

Finally, we check for any discrepancies within the data frames.

```{r}
#| eval: false
c(setdiff(vaccination$KELURAHAN, dki$DESA_KELUR), setdiff(dki$DESA_KELUR, vaccination$KELURAHAN))
```

It seems like we are clear to merge the data frames again.

```{r}
dataMerged = merge(dki, vaccination_pivot, 
             by.x = "DESA_KELUR", by.y = "KELURAHAN", 
             all = TRUE)
```

```{r}
#| eval: false
any(duplicated(dataMerged))
```

```{r}
#| eval: false
any(is.na(dataMerged[-c(268:269),]))
```

```{r}
#| eval: false
any(st_is_empty(dataMerged))
```

It looks like our data frame dataMerged is now cleaned. We can now move on to the last steps before the assignments' objectives.

## Data visualization

```{r}
tmap_mode("plot")
qtm(dataMerged)
```

It looks like the Pulau Islands are included in our data frame. We will take the necessary steps to remove them from the sf data frame.

## Data cleaning

```{r}
dataCleaned = dataMerged %>% 
  filter(!grepl('PULAU', DESA_KELUR))
```

```{r}
tmap_mode("plot")
qtm(dataCleaned)
```

We can now start answering the objectives of the Hands-on Exercise 2.

# Choropleth Mapping and Analysis

Objectives of this section:

-   Compute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,

-   Prepare the monthly vaccination rate maps by using appropriate tmap functions,

-   Describe the spatial patterns revealed by the choropleth maps (not more than 200 words).

## Monthly Vaccination Rate Computation

```{r}
vac_minus1 = dataCleaned %>%
  extract2(colnames(dataCleaned)[grepl("2021-06-30" , names(dataCleaned))])
vac_target = dataCleaned %>%
  extract2(colnames(dataCleaned)[grepl("Target" , names(dataCleaned))])

for (i in 12:23){
  
  vac_current = dataCleaned %>%
  extract2(colnames(dataCleaned)[i])
  
  colname = paste("Vaccination_Rate_for", months[i-11], sep = "_")
  dataCleaned[colname] = (vac_current - vac_minus1) / vac_target
  
  vac_minus1 = vac_current
}

dataCleaned = dataCleaned[, !grepl("Yet-to-be", colnames(dataCleaned))]
```

## Monthly Vaccination Rate Visualization

```{r}
vac_rate = c(colnames(dataCleaned)[grepl("Vaccination_Rate" , colnames(dataCleaned))])

tm_shape(dataCleaned) +
  tm_fill(vac_rate, 
          style = "jenks",
          palette = "Blues") +
  tm_layout(title = gsub("_", " ", vac_rate),
            title.position = c("center", "top"),
            legend.position = c("left", "bottom"),
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="4star", size = 2) +
  tm_scale_bar(width = 0.15) +
  tm_grid(lwd = 0.1, alpha = 0.2)
```

## Conclusion

here...

# Local Gi* Analysis

Objectives of this section:

-   Compute local Gi* values of the monthly vaccination rate,

-   Display the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05),

-   With reference to the analysis results, draw statistical conclusions (not more than 250 words).

## Compute local Gi* values

### Preliminary work

We shall start by dropping the rows containing the TOTAL string in the KELURAHAN column.

```{r}
vaccination = vaccination %>%
  filter(!grepl('TOTAL', KELURAHAN))
```

### Convertion to spacetime cube

We will create a space time cube since we are looking at values in a time series.

```{r}
dataMerged_st <- spacetime(vaccination, dki,
                        .loc_col = "GeoCode",
                        .time_col = "Date")
```

```{r}
is_spacetime_cube(dataMerged_st)
```

The merged data does not seem to be in the space time cube format. It looks like we should take the NA values out of the dki data frame.

```{r}
dki = na.omit(dki)
```

Let's run the code chunk again.

```{r}
dataMerged_st <- spacetime(vaccination, dki,
                        .loc_col = "GeoCode",
                        .time_col = "Date")
```

```{r}
is_spacetime_cube(dataMerged_st)
```

It looks like the transformation is successful. Now, we shall continue with the computation of Gi*.

### Data cleaning

We shall compute the monthly vaccination rates.

```{r}
dataMerged_st = dataMerged_st[order(dataMerged_st$KELURAHAN),] 

dataMerged_st["Monthly_vac_rate"] = (lag(dataMerged_st[,4]) - dataMerged_st[, 4]) / dataMerged_st$Target
```

Now we shall drop all the rows containing data of 2021-06-30.

```{r}
dataMerged_st = dataMerged_st %>%
  filter(!grepl(as.Date("2021-06-30"), Date))
```

### Deriving the spatial weights

```{r}
data_nb <- dataMerged_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

### Compute Gi* values

```{r}
gi_stars <- data_nb %>% 
  group_by(Date) %>% 
  mutate(gi_star = local_gstar_perm(
    Monthly_vac_rate, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

## Gi* map of monthly vaccination rate

```{r}
dki = dki %>% 
  filter(!grepl('PULAU', DESA_KELUR)) 
```


```{r}
test = merge(dki, gi_stars) %>%
  filter(p_value < 0.05)
```

```{r}
tm_shape(dki) +
  tm_polygons() +
tm_shape(test) +
  tm_fill("p_value",
          n = 1,
          breaks = c(0, 0.05),
          palette = "red") + 
  tm_facets(by="Date", 
            free.coords=TRUE, 
            drop.shapes=TRUE) +
  tm_layout(legend.show = FALSE,
            title.position = c("center", "center"), 
            title.size = 20) +
  tm_borders(alpha = 0.5)
```

## Conclusion

conclude...

# Emerging Hot Spot Analysis (EHSA)

Objectives of this section:

-   Perform Mann-Kendall Test by using the spatio-temporal local Gi* values,

-   Select three sub-districts and describe the temporal trends revealed (not more than 250 words),

-   Prepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05),

-   With reference to the EHSA map prepared, describe the spatial patterns revealed (not more than 250 words).

## Mann-Kendall Test

```{r}
cbg <- gi_stars %>% 
  ungroup() |> 
  select(KELURAHAN, Date, gi_star)
```

```{r}
p <- ggplot(data = cbg, 
       aes(x = Date, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

```{r}
ehsa <- gi_stars %>%
  group_by(KELURAHAN) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

## Temporal Trends in ...

describe

## EHSA map of Gi* values

```{r}
ehsa = emerging_hotspot_analysis(
  x = dataMerged_st, 
  .var = "Monthly_vac_rate", 
  k = 1, nsim = 99)
```

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

```{r}
dki_ehsa <- dki %>%
  left_join(ehsa, by = c("GeoCode" = "location"))
```

```{r}
ehsa_sig <- dki_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(dki_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```


## Spatial Patterns description
