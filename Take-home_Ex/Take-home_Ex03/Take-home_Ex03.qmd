---
title: "Predicting HDB Public Housing Resale Prices using Geographically Weighted Methods"
subtitle: "Take-home Exercise 3"

author: "Pierre HAAS"

date: "March 07, 2023"
date-modified: "`r Sys.Date()`"

execute:
  eval: true
  echo: true
  warning: false
editor: visual

number-sections: true
---

# Getting Started

In this Take-home Exercise 3, we will try using geographically weighted methods to predict the resale price of HDB public housing in Singapore. You will find in the next to sections a brief description of the data sets used and where to retrieve them as well as the list of all the libraries needed for this project and a description of their specific usage.

## Installing and Loading Packages

For the purpose our this project, we have selected a list of libraries that will allow to perform all the necessary data cleaning, handling and analysis.

The following list shows the libraries that will be used in this assignment:

*pacman, tidyverse, janitor, sf, spdep, tmap, corrplot, olsrr, knitr, httr, onemapsapi, xml2, rvest, stringr, ggmap, ggpubr, GWmodel, SpatialML, Metrics*

To install and load the packages, we use the p_load() function of the pacman package. It automatically checks if packages are installed, installs them if they are not installed, and loads the packages into the R environment.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep,
               GWmodel, tmap, tidyverse, gtsummary,
               ranger, SpatialML, janitor, stargazer, 
               lmtest, Metrics)
```

```{r}
#| eval: false
pacman::p_load(knitr, httr, onemapsgapi, xml2, rvest,
               stringr, ggmap)
```

```{r}
#| eval: false
#| echo: false
library(tidygeocoder)
```

```{r}
options(digits = 15)
```

## The Data

For the purpose of this assignment, extensive amount of data was extracted from the web. You will find below a table that directs you to the webpage on which you can retrieve the data sets.

|           Name           |     Format     |                                                      Source                                                      |
|:------------------------:|:--------------:|:----------------------------------------------------------------------------------------------------------------:|
|    Resale Flat Prices    |      csv       |                          [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)                           |
| Master Plan 2019 Subzone |      shp       |                                                       prof                                                       |
|    MRT Exit Locations    |      shp       | [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/dam/datamall/datasets/Geospatial/TrainStationExit.zip) |
|    Bus Stop Locations    |      shp       | [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/dam/datamall/datasets/Geospatial/BusStopLocation.zip)  |
|       Supermarkets       |      kml       |                             [data.gov.sg](https://data.gov.sg/dataset/supermarkets)                              |
|     Primary Schools      | HTML scrapping |                                     [Here](https://sgschooling.com/school/)                                      |
|      Shopping Malls      | HTML scrapping |                  [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore)                  |
|     Rest of the data     |      API       |                                   [OneMapAPI](https://www.onemap.gov.sg/docs/)                                   |

# Retrieving and Importing the Geospatial Data

In this first section that addresses the data import, we will be going through the necessary work to load the geospatial data correctly into our R environment.

Note that in this section, we will be load data previously retrieved from the web, but also using some APIs to collect necessary variables that will play an important role in our regression models.

## Master Plan 2019 with sub-zones

::: panel-tabset
### Importing the data

The first geospatial data set that we are importing is the 2019 Master Plan with sub-zones. Since it is a shp file, we use the st_read() function of the sf package to properly import the data frame.

```{r}
mpsz.sf = st_read(dsn = "data/geospatial/mpsz2019",
                  layer = "MPSZ-2019")
```

The import was successful, however, before moving on I would like to check if all geometries are valid, and also if the projection code is correectly encoded.

```{r}
any(st_is_valid(mpsz.sf) == FALSE)
```

It seems like we have invalid geometries. Now, let's check the CRS code of the data frame.

```{r}
st_crs(mpsz.sf)
```

It looks like the EPSG and coordinate system does not correspond to the Singaporean SVY21. So, we will change that. We pipe the st_transform() and st_make_valid() functions to solve the two issues we had just pointed out.

```{r}
mpsz.sf = mpsz.sf %>%
  st_make_valid() %>%
  st_transform(3414)
```

```{r}
any(st_is_valid(mpsz.sf) == FALSE)
```

```{r}
st_crs(mpsz.sf)
```

The two issues seem to be solved. Note that I show you how to proceed when importing geospatial data here but will not do so in such an extensive manner later. My goal here is to show you how to replicate my way of proceeding when handling the data for the first time. To make the reading easier, I have cut out all my exploration of this webpage and left only the essential and necessary code.

### Visualizing the data

Using the qtm() function of the tmap package, we can make a quick visualization of the geometry stored in the sf data frame.

```{r}
qtm(mpsz.sf)
```
:::

## MRT Train stations

Importing MRT Exits into an sf data frame.

::: panel-tabset
### Importing the data

As explained just before, I will be sticking with the necessary code only from now on. Using the following code chunk, we import the MRT Train Stations into an sf data frame.

```{r}
mrt_exits.sf = st_read(dsn = "data/geospatial/TrainStationExit/TrainStationExit",
                       layer = "Train_Station_Exit_Layer") %>%
  st_transform(crs=3414)
```

### Visualizing the data

Using the tmap package, we can conveniently plot the MRT Exits on the Singaporean map.

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(mrt_exits.sf)+
  tm_dots(alpha = 0.5)
```
:::

## Bus Stop Locations

Importing Bus Stops into an sf data frame.

::: panel-tabset
### Importing the data

```{r}
bus_stops.sf = st_read(dsn = "data/geospatial/BusStopLocation/BusStop_Feb2023",
                       layer = "BusStop") %>%
  st_transform(3414)
```

### Visualizing the data

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(bus_stops.sf)+
  tm_dots(alpha = 0.5)
```
:::

## Supermarket data

Importing Supermarket Locations into an sf data frame.

::: panel-tabset
### Importing the data

```{r}
supermarkets.sf = st_read("data/geospatial/supermarkets/supermarkets-kml.kml") %>%
  st_transform(3414)
```

### Visualizing the data

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(supermarkets.sf)+
  tm_dots(alpha = 0.5)
```
:::

## Retrieving the independent variables using the OneMap API

We will be using the One Map Singapore API to retrieve some of the independent variables that we will be using later on in our regression model.

We will be taking a few preliminary steps before extracting the variables. You will find everything you need to replicate my work in the following sections.

### Retrieving the API token using the *onemapsgapi* package

The first step to getting the API key is to create an account on the One Map website (click [here](https://www.onemap.gov.sg/docs/#register-free-account) to access the registration tutorial).

Now that you are registered, we may retrieve the API key using the *onemapsgapi* package and its get_token() function. By simply entering your registration email and password, you will get a valid API key for a three-day period.

Note that if you want to learn more about the One Map API, you should definitely check their [website](https://www.onemap.gov.sg/docs/) and their [web app](https://app.swaggerhub.com/apis/onemap-sg/new-onemap-api/1.0.4) that allows to test the API's services.

```{r}
#| eval: false
token_api = get_token(email = "xxx@xxx.xxx", 
                      password = "...")
```

### Using the API to retrieve the variables available on OneMap

The first step to retrieving our independent variables is to look at the available data provided by One Map. We do so using the httr package that allows us to make HTML queries. Using the GET() function, we retrieve every theme info of the OneMap API.

```{r}
#| eval: false
url = "https://developers.onemap.sg/privateapi/themesvc/getAllThemesInfo"

query <- list("token" = token_api)
result <- GET(url, query = query)
```

Using the content() function, we may look at the information retrieved, however, you should be made aware that the result from our GET() command is quite messy. So, you will see in the next section how we extract the theme names and store them into a list.

```{r}
#| eval: false
content(result)
```

### Creating the variable list

As explained, the information retrieved is quite messy. So, we will be storing in a list the essential information that we will use later on.

Using a for loop, I store the theme specific information into a list called themes_list.

```{r}
#| eval: false
themes_list = list()

for (i in 1:length(content(result)$Theme_Names)){
  themes_list = append(themes_list, content(result)$Theme_Names[[i]])
}
```

To take a look at the information stored into our list, you may use the next code chunk. However, since the list is quite long, I won't print the output/information here and will directly give you a list you the variables I have extracted.

```{r}
#| eval: false
themes_list
```

In the next code chunk, you may see the variables that I carefully selected. They give information about: Eldercare Services, Hawker Centres, Food Courts, Parks in Singapore, Kindergartens, and Childcare Centres.

```{r}
#| eval: false
ind_variables = c("eldercare", "hawkercentre", "hawkercentre_new", "healthier_hawker_centres", "maxwell_fnb_map", "healthiercaterers", "relaxsg", "nationalparks", "kindergartens", "childcare")
```

I have also picked-up some other interesting variables that may be included later in my regression model to improve the quality of the future model.

```{r}
#| eval: false
extra_variables = c("lta_cycling_paths", "disability", "hsgb_fcc", "sportsg_sport_facilities", "ssc_sports_facilities", "wastedisposalsite", "drainageconstruction", "cpe_pei_premises", "sewerageconstruction", "danger_areas", "aquaticsg", "moh_isp_clinics", "heritagetrees", "nparks_activity_area",  "exercisefacilities", "mha_uav_2015", "playsg", "underconstruction_facilities", "preschools_location", "hdbcyclingunderconstruction",  "boundary_5km",  "hdbluppropunderconstruction", "parkingstandardszone", "libraries", "cyclingpath", "dengue_cluster", "greenbuilding", "nparks_uc_parks", )
```

You won't see these additional variables in my model due to the computing power of my computer. However, I strongly recommend to try an include these potential independent variables to improve the model.

### Retrieving the available variables with the OneMap API

Using the code chunk below, you can retrieve the location name and its coordinates based on the previously created list of variables. We store this information into a data frame to make our use of this data easier

```{r}
#| eval: false
df = data.frame()

url = "https://developers.onemap.sg/privateapi/themesvc/retrieveTheme"

for (x in ind_variables){
  
  query <- list("queryName" = x, "token" = token_api)
  
  result <- GET(url, query = query)
  
  print("Start")
  print(x)
  
  for (i in 2:length(content(result)$SrchResults)){
    new_row = c(content(result)$SrchResults[[i]]$NAME,
                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, ",", 2)[1],
                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, ",", 2)[2],
                content(result)$SrchResults[[1]]$Theme_Name)
    
    df = rbind(df, new_row)
  }

}

colnames(df)<-c("location_name", "lat", "lon", "variable_name")
```

Note that retrieving the data takes a few minutes, so I took care of exporting the data frame into a csv file to avoid running the previous code chunk multiple times.

```{r}
#| eval: false
write_csv(df, "data/geospatial/retrieved_variables.csv")
```

We now have part of our independent variables stored into a csv file that we will import back into our R environment.

```{r}
retrieved_variables = read_csv("data/geospatial/retrieved_variables.csv", )
```

If we look back to the variables we chose to retrieve using the One Map API, we had sub-categories to our independent variables.

```{r}
unique(retrieved_variables$variable_name)
```

For instance, we have four sub-categories for hawker centres and food courts. We shall simplify our work by renaming these sub-categories under one name only. Using the next code chunk, we may do so.

```{r}
retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Eldercare Services", 
                                                "eldercare")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "^Hawker Centres$", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Hawker Centres_New", 
                                                "hawker_centres")


retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Healthier Hawker Centres", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Maxwell Chambers F&B map", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Healthier Caterers", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Parks@SG", 
                                                "parks")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Parks", 
                                                "parks")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Kindergartens", 
                                                "kindergartens")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Child Care Services", 
                                                "child_care")
```

```{r}
unique(retrieved_variables$variable_name)
```

We shall now proceed to creating sf objects from the extracted coordinates.

### Transforming the data frame to sf data frame

Using the st_as_sf() function from the *sf* package, we can transform our set of coordinates into sf point objects.

```{r}
retrieved_variables.sf = st_as_sf(retrieved_variables,
                                  coords = c("lon", "lat"),
                                  crs = 4326) %>%
  st_transform(3414)
```

We shall check if our data points have been correctly transformed by plotting the points on the Singapore map using the *tmap* package.

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(retrieved_variables.sf)+
  tm_dots()
```

# Retrieving and Importing the Aspatial Data

## Resale Price of HDB flats

After retrieving the resale HDB prices from the data.gov.sg website, I moved the folder into my data folder, it contains all the data sets necessary for our analysis, and more precisely stored it into the aspatial folder.

The downloaded data is stored in a folder called resale-flat-prices, itself containing multiple files. To choose the right file, we can use the base r function list.files() to return a list of the file's names.

```{r}
path = "data/aspatial/resale-flat-prices"
files = list.files(path)
files
```

Looking at the above list, it seems clear that we should use file n°5 as it contains the data from 2017 and later. It gives us the intuition that we may need to treat the data and take a subset of data rows within the analysis period (2021 to 2023). We will import the data using the read_csv() function from the readr package.

```{r}
resale_price = read_csv(file = paste(path, files[5], sep = "/"))
```

We should take a quick look at the fields contained in the resale_price data frame. We do so using the glimpse() function.

```{r}
glimpse(resale_price)
```

The available fields are as follows:

-   month: it indicates the month of transaction
-   town: it indicates the district
-   flat_type: it indicates the number of rooms in the flat
-   block: it indicates the block number of the HDB flat
-   street_name: it indicates the street number and name
-   storey_range: it indicates the floor number of the flat
-   floor_area_sqm: it indicates the floor area of the flat in square meter
-   flat_model: it indicates the HDB flat model
-   lease_commence_date: it indicates the starting year date of the lease (in Singapore, HDB flats are 99-year leaseholds)
-   remaining_lease: it indicates the remaining years and months on the 99-year leasehold
-   resale_price: it indicates the resale price of a given flat in SGD (Singapore dollars)

Now, we will use the head() function to look at the available data in each field.

```{r}
head(resale_price)
```

### Selecting trasactions of 3 Room apartments from 2021 onwards

My previously mentioned intuition is confirmed. The month variable takes data starting in January 2017 and, through quick data manipulation, we will have to restrain the data frames to the 2021-2022 analysis period and 2023 data testing period. In addition, it looks like the town field in unnecessary and may be dropped. We only need the block and steet_name fields to geocode the addresses and create sf geometry.

Here we pipe the select() function to remove the town variable, filter and grepl functions to create a subset of the data by only selecting transactions that took place in 2021 to 2023, and finally pipe once again the filter function to select only three room HDB flats.

```{r}
rp_3rooms = resale_price %>%
  select(-2) %>%
  filter(grepl("202[123]", month)) %>%
  filter(flat_type == "3 ROOM")
```

### Transforming the storey_range column to dummy variables

Since we would like to include the storey_range variable in our analysis and the column only takes categorical data, we shall create dummy variables to indicate the storey range of each HDB flat.

The first step is to look at what are the unique values in the column. Using the unique() function, we retrieve in the form of a list the unique categorical values.

```{r}
unique(rp_3rooms$storey_range)
```

Now, using a for loop and the ifelse() function, we will create a new column for each unique categorical variable contained in the storey_range field and assign a 1 if the particular HDB flat belongs to the specific storey range.

```{r}
#| eval: false
for (i in unique(rp_3rooms$storey_range)){
  rp_3rooms[i] = ifelse(rp_3rooms$storey_range == i, 1, 0)
}
```

### Transforming the flat_model column to dummy variables

We repeat the same process with the flat_model variable.

```{r}
unique(rp_3rooms$flat_model)
```

```{r}
#| eval: false
for (i in unique(rp_3rooms$flat_model)){
  rp_3rooms[i] = ifelse(rp_3rooms$flat_model == i, 1, 0)
}
```

### Transforming the remaining_lease to a numerical variable

If you remember previously when we took a look at the different data fields available in our data frame, you could notice that the remaining_lease variable stored character data, however, we would like it to be a numerical data field.

To transform our column into a numerical field, we use a for loop with if/else statements. This will allow us to extract the number of years and months remaining on the lease and create a variable that stores the years remaining before expiration of the lease.

```{r}
#| eval: false
lease_remaining = list()

for (i in 1:nrow(rp_3rooms)){
  
  lease = str_extract_all(rp_3rooms$remaining_lease, "[0-9]+")[[i]]
  
  year = as.numeric(lease[1])
  
  if (length(lease) < 2){
    
    lease_remaining = append(lease_remaining, year)
  
    } else {
      
    month = as.numeric(lease[2])
    
    lease_remaining = append(lease_remaining, round(year+month/12, 2))
  
    }

}
```

```{r}
#| eval: false
rp_3rooms$remaining_lease = as.numeric(lease_remaining)
```

### Geocoding the address

The first step to geocoding the address - retrieving the latitude and longitude based in the address - is to create a new column that stores the cleaned full address. Consequently, we create a new field called cleaned_address that combines both the block and street_name fields. This will allow us to retrieve the geocode of the HDB flats in our query.

```{r}
#| eval: false
rp_3rooms["cleaned_address"] = paste(rp_3rooms$block, rp_3rooms$street_name, sep = " ")
```

Now that we have a column that stores the cleaned addresses, we can move on to the geocoding. By using the httr package and One Map API, we will create GET requests that retrieve the full address of the HDB flat, its longitude and latitude. We will store all this data into three lists to later create three additional columns in our data frame.

```{r}
#| eval: false
url = "https://developers.onemap.sg/commonapi/search"

full_address = list()
latitude = list()
longitude = list()

for (address in rp_3rooms$cleaned_address){
  query <- list("searchVal" = address,
                "returnGeom" = "Y", "getAddrDetails" = "Y")
  res <- GET(url, query = query)
  
  full_address = append(full_address, content(res)$results[[1]]$ADDRESS)
  latitude = append(latitude, content(res)$results[[1]]$LATITUDE)
  longitude = append(longitude, content(res)$results[[1]]$LONGITUDE)
}

```

```{r}
#| eval: false
rp_3rooms$full_address = full_address
rp_3rooms$lat = latitude
rp_3rooms$lon = longitude
```

Before exporting the data frame to a csv file (to avoid repeating this lengthy step), we shall use the apply function to transform our three newly created columns to the right type (character / numerical type).

```{r}
#| eval: false
options(digits=15)

rp_3rooms$full_address = apply(rp_3rooms[, 28], 2, as.character)
rp_3rooms$lat = apply(rp_3rooms[, 29], 2, as.numeric)
rp_3rooms$lon = apply(rp_3rooms[, 30], 2, as.numeric)
```

```{r}
#| eval: false
write.csv(rp_3rooms, "data/aspatial/geocoded_resale_price.csv", row.names = FALSE)
```

We import the geocoded data back into the R environment.

```{r}
rp_3rooms = read_csv("data/aspatial/geocoded_resale_price.csv")
```

### Dropping the irrelevant variables

```{r}
head(rp_3rooms)
```

We will drop the following irrelevant fields.

```{r}
rp_3rooms.cleaned = rp_3rooms %>%
  select(-c("flat_type", "block", "street_name", "storey_range", "flat_model", "cleaned_address", "full_address"))
```

### Creating the sf geometry

Using the tibble data frame and lon/lat fields, we create a sf data frame and visualize the data with the tmap package.

::: panel-tabset
#### Creating the sf data frame

```{r}
rp_3rooms.sf = st_as_sf(rp_3rooms.cleaned,
                        coords = c("lon", "lat"),
                        crs = 4326) %>%
  st_transform(3414)
```

#### Visualizing the data

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(rp_3rooms.sf)+
  tm_dots()
```
:::

## Primary School data

After thoroughly looking on the web, I was not able to find a list of primary schools of Singapore to download. So, to improve the reproducibility of my work, I scrapped the [sgschooling](https://sgschooling.com/school/) website to retrieve what seemed to me like an up-to-date list of primary schools.

### Scrapping Primary Schools online with *rvest* package

The first step is to retrieve the names of the primary schools. Looking at the code chunk below, I used the *httr*, *xml2*, and *rvest* packages to create a GET request, read the HTML code of the website, and extract the names of the schools to store them into a list.

```{r}
#| eval: false
url = "https://sgschooling.com/school/"

res = GET(url)

wiki_read <- xml2::read_html(res, encoding = "UTF-8")

testt = wiki_read %>%
  html_elements("a")

testt1 = testt[grepl("school/", testt)]

pattern <- ">(.*)<"
result <- regmatches(testt1, regexec(pattern, testt1))

schools = list()

for (i in 1:length(result)){
  new_row = result[[i]][2]
  schools = append(schools, new_row)
}

```

### Using the OneMap API to retrieve the schools' coordinates

I then used this list and the One Map API to geocode these schools and retrieve their address as well as their longitude and latitude, and then stored all of it into a data frame.

Note that in my code I include an if/else statement in the case where we do not get a perfect match and are not able to retrieve information with the API.

```{r}
#| eval: false
primary_sc = data.frame()

url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:length(schools)){
  
  query <- list("searchVal" = schools[[i]],
                "returnGeom" = "Y", 
                "getAddrDetails" = "Y")
  result <- GET(url, query = query)
  
  if (length(content(result)$results) == 0){
    new_row = c(schools[[i]], 
              NA,
              NA,
              NA)
    primary_sc = rbind(primary_sc, new_row)
    
  } else{
    new_row = c(schools[[i]], 
                content(result)$results[[1]]$ADDRESS,
                content(result)$results[[1]]$LATITUDE,
                content(result)$results[[1]]$LONGITUDE)
    
    primary_sc = rbind(primary_sc, new_row)
    
  }

}

colnames(primary_sc) = c("location_name", "full_address", "lat", "lon")
```

### Checking for missing information

```{r}
#| eval: false
sum(is.na(primary_sc$full_address))
```

It seems like we have missing data for 9 primary schools. It is probably because the name of the school is not recognized by the API, so we will try to clean them to get their information into our data frame.

```{r}
#| eval: false
primary_sc %>%
  filter(is.na(primary_sc$full_address) == TRUE) %>%
  select(1)
```

By checking on the web, it seems like the "Juying Primary School" has merged with the "Pioneer Primary School", so we may drop the "Juying Primary School" from the data frame.

```{r}
#| eval: false
primary_sc1 = primary_sc %>%
  filter(!grepl("Juying Primary School", location_name))
```

Now, regarding the remaining schools, after performing a tests, it seems like I am not able to retrieve the information using the API so we will be filling the values with, first, the Google Maps API and then, if there are still some empty information, I will be filling the values manually by looking online.

You may want to check the next two code chunks I have used to try and retrieve the missing information using the OneMap API.

```{r}
#| eval: false
primary_sc1$location_name = str_replace(primary_sc1$location_name, 
                                        "St. ", "")

# I have also tried replacing the string with: "Saint" and "St" but I am not able to make the API retrieve the data
```

```{r}
#| eval: false
url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:nrow(primary_sc1)){
  
  if (is.na(primary_sc1[i, 2])){
    query <- list("searchVal" = primary_sc1[i, 1],
                  "returnGeom" = "Y", 
                  "getAddrDetails" = "Y")
    result <- GET(url, query = query)
    
    if (length(content(result)$results) == 0){
      next
      
      } else{
        primary_sc1[i, 2] = content(result)$results[[1]]$ADDRESS
        primary_sc1[i, 3] = content(result)$results[[1]]$LATITUDE
        primary_sc1[i, 4] = content(result)$results[[1]]$LONGITUDE
    
  }} else{ next
  
  }
}
```

These are the schools with the missing information.

```{r}
#| eval: false
primary_sc1 %>%
  filter(is.na(primary_sc1$full_address) == TRUE) %>%
  select(1)
```

Using the Google Maps API and its library *ggmap*, we will loop through the schools with missing coordinates. Since all these schools are located consecutively in the data frame, I use the following structure to create the list of schools.

While looping through these schools, we use the geocode() function of *ggmap* to retrieve the latitude and longitude based on the school name and assign these values in the corresponding missing cells of the primary_sc1 data frame.

```{r}
#| eval: false
register_google(key = "xxx")

missing_sc = primary_sc1[grep("St. Andrew’s Junior School", primary_sc1$location_name):grep("St. Stephen’s School", primary_sc1$location_name), 1]

for (i in missing_sc){
  coords = geocode(i)
  primary_sc1[grep(i, primary_sc1$location_name), 3] = coords$lat
  primary_sc1[grep(i, primary_sc1$location_name), 4] = coords$lon
  
}
```

We should check if we still have missing coordinates. Note that we are missing information in the full_address column, however, it is not a great deal since we will be using the coordinates to create the sf objects later on.

```{r}
#| eval: false
primary_sc1 %>%
  filter(is.na(primary_sc1$lat) == TRUE)
```

We are still missing coordinates for three schools. We will be filling these values manually.

```{r}
#| eval: false
primary_sc1[grep("St. Anthony’s Primary School", primary_sc1$location_name), 3:4] = list(1.3796337949311097, 103.75033229288428)

primary_sc1[grep("St. Gabriel’s Primary School", primary_sc1$location_name), 3:4] = list(1.3499742816603595, 103.86268328706147)

primary_sc1[grep("St. Stephen’s School", primary_sc1$location_name), 3:4] = list(1.3189794629543288, 103.9179118196036)
```

We are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).

```{r}
#| eval: false
write.csv(primary_sc1, "data/aspatial/primary_schools.csv", row.names = FALSE)
```

```{r}
primary_schools = read_csv("data/aspatial/primary_schools.csv")
```

### Creating the sf data frame

```{r}
primary_schools.sf = st_as_sf(primary_schools,
                         coords = c("lon", "lat"),
                         crs = 4326) %>%
  st_transform(3414)
```

### Visualizing the data

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(primary_schools.sf)+
  tm_dots()
```

### Creating a field with the Top 20 Primary Schools

We will now create an additional variable that indicates whether one primary school is considered good or not. I made the selection factor be whether the primary school is in the Top 20 primary schools or not. I extracted from the internet the Top 20 list that you may find below. With this list, we can create a dummy variable that stores a 1 (0) if the school is part (or not) of the Top 20.

```{r}
#| eval: false
top20_sc = c("Methodist Girls’ School (Primary)", "Catholic High School", "Tao Nan School", "Pei Hwa Presbyterian Primary School", "Holy Innocents’ Primary School", "Nan Hua Primary School", "CHIJ St. Nicholas Girls’ School", "Admiralty Primary School", "St. Hilda’s Primary School", "Ai Tong School", "Anglo-Chinese School (Junior)", "Chongfu School", "St. Joseph’s Institution Junior", "Anglo-Chinese School (Primary)", "Singapore Chinese Girls’ Primary School", "Nanyang Primary School", "South View Primary School", "Pei Chun Public School", "Kong Hwa School", "Rosyth School")

primary_schools.sf["top20"] = ifelse(primary_schools.sf$location_name %in% top20_sc == TRUE,
                                     1, 0)
```

Note that I originally forgot to create this variable and am doing it after having ran all the regressions. Unfortunately, since my computer crashes most of the time when trying to perform the geographic regression, I won't be able to include it now, but I still made sure to include how to create this variable.

## Shopping Mall data

We encounter the same problem with the shopping malls as when looking for a data set of primary schools, so I decided to scrape Wikipedia for this one.

We perform very similar steps as the previous section on primary schools, so I won't run you through every step.

### Scraping Wikipedia to extract the list of malls in Singapore

```{r}
#| eval: false
url = "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"

res = GET(url)

wiki_read <- xml2::read_html(res, encoding = "UTF-8")

testt = wiki_read %>%
  html_nodes("div.div-col") %>%
  html_elements("ul") %>%
  html_elements("li") %>%
  html_text()

malls = str_replace(testt, "\\[.*]", "")

```

### Using the OneMap API to retrieve the malls' coordinates

```{r}
#| eval: false
shp_malls = data.frame()

url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:length(malls)){
  
  query <- list("searchVal" = malls[i],
                "returnGeom" = "Y", 
                "getAddrDetails" = "Y")
  result <- GET(url, query = query)
  
  if (length(content(result)$results) == 0){
    new_row = c(malls[i], 
              NA,
              NA,
              NA)
    shp_malls = rbind(shp_malls, new_row)
    
  } else{
    new_row = c(malls[i], 
                content(result)$results[[1]]$ADDRESS,
                content(result)$results[[1]]$LATITUDE,
                content(result)$results[[1]]$LONGITUDE)
    
    shp_malls = rbind(shp_malls, new_row)
    
  }

}

colnames(shp_malls) = c("location_name", "full_address", "lat", "lon")
```

```{r}
#| eval: false
sum(is.na(shp_malls$full_address))
```

### Filling missing information with ggmap

```{r}
#| eval: false
shp_malls %>%
  filter(is.na(shp_malls$full_address) == TRUE) %>%
  select(1)
```

Before using the Google Maps API, I decided to research about these nine malls and found that:

-   The PoMo mall is now called GR.iD, so we shall change the name in the data frame

-   The KINEX mall is found under the name KINEX, so we shall remove the additional information from the name

-   The Paya Lebar Quarter mall is found under the name PLQ mall on Google Maps, so we shall change the name in the data frame

```{r}
#| eval: false
shp_malls[grep("PoMo", shp_malls$location_name), 1] = "GR.iD";

shp_malls[grep("KINEX", shp_malls$location_name), 1] = "KINEX";

shp_malls[grep("PLQ", shp_malls$location_name), 1] = "PLQ mall"
```

```{r}
#| eval: false
register_google(key = "xxx")

missing_malls = shp_malls %>%
  filter(is.na(shp_malls$full_address) == TRUE) %>%
  `$`(location_name)

for (i in list(missing_malls)[[1]]){
  coords = geocode(i)
  shp_malls[grep(i, shp_malls$location_name), 3] = coords$lat
  shp_malls[grep(i, shp_malls$location_name), 4] = coords$lon
  
}
```

```{r}
#| eval: false
shp_malls %>%
  filter(is.na(shp_malls$lat) == TRUE)
```

### Filling missing information manually

We are still missing coordinates of the KINEX mall. We will be filling these values manually.

```{r}
#| eval: false
shp_malls[grep("KINEX", shp_malls$location_name), 3:4] = c(1.314893715213727, 103.89480904154526)
```

We are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).

```{r}
#| eval: false
write.csv(shp_malls, "data/aspatial/shopping_malls.csv", row.names = FALSE)
```

```{r}
shp_malls = read_csv("data/aspatial/shopping_malls.csv")
```

### Creating an sf data frame

```{r}
shp_malls.sf = st_as_sf(shp_malls,
                        coords = c("lon", "lat"),
                        crs = 4326) %>%
  st_transform(3414)
```

### Visualizing the data

```{r}
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(shp_malls.sf)+
  tm_dots()
```

# Preparing the final data frame

In this section, we will prepare the following variables:

Proximity to CBD ; Proximity to eldercare ; Proximity to foodcourt/hawker centres ; Proximity to MRT ; Proximity to park ; Proximity to good primary school ; Proximity to shopping mall ; Proximity to supermarket

Numbers of kindergartens within 350m ; Numbers of childcare centres within 350m ; Numbers of bus stop within 350m ; Numbers of primary school within 1km

## Proximity variables

### Proximity to CBD

We will consider the Downtown Core planning area to be the Central Business District. Given the previous statement, we will compute the distance of each HDB flats to the CBD area.

The first step is to compute the nearest feature between the HDB flats and CDB area. With this, we then can create a column that stores the distance between the two features (e.g., distance between each flat and nearest part of the CBD).

```{r}
#| eval: false
cbd_zone.sf = mpsz.sf %>%
  filter(PLN_AREA_N == "DOWNTOWN CORE")

nearest = st_nearest_feature(rp_3rooms.sf, cbd_zone.sf)

rp_3rooms.sf$distance_to_cdb = as.numeric(
  st_distance(rp_3rooms.sf, 
              cbd_zone.sf[nearest,],
              by_element=TRUE) )
```

Note that I had forgot to compute this variable when running my regressions, so you won't see it later but I made sure to include the code to compute it.

### Proximity to eldercare

```{r}
#| eval: false
eldercare.sf = retrieved_variables.sf %>%
  filter(variable_name == "eldercare")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, eldercare.sf)

rp_3rooms.sf$distance_to_eldercare = as.numeric(
  st_distance(rp_3rooms.sf, 
              eldercare.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to food court / hawker centres

```{r}
#| eval: false
hawker_centres.sf = retrieved_variables.sf %>%
  filter(variable_name == "hawker_centres")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, hawker_centres.sf)

rp_3rooms.sf$distance_to_food = as.numeric(
  st_distance(rp_3rooms.sf, 
              hawker_centres.sf[nearest,], 
              by_element=TRUE) )
```

### Proximity to MRT

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, mrt_exits.sf)

rp_3rooms.sf$distance_to_mrt = as.numeric(
  st_distance(rp_3rooms.sf, 
              mrt_exits.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to park

```{r}
#| eval: false
parks.sf = retrieved_variables.sf %>%
  filter(variable_name == "parks")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, parks.sf)

rp_3rooms.sf$distance_to_park = as.numeric(
  st_distance(rp_3rooms.sf, 
              parks.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to good primary school

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, primary_schools.sf)

rp_3rooms.sf$top20_schools = as.numeric(
  st_distance(rp_3rooms.sf, 
              primary_schools.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to shopping mall

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, shp_malls.sf)

rp_3rooms.sf$distance_to_mall = as.numeric(
  st_distance(rp_3rooms.sf, 
              shp_malls.sf[nearest,], 
              by_element=TRUE) )
```

### Proximity to supermarket

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, supermarkets.sf)

rp_3rooms.sf$distance_to_supermarkets = as.numeric(
  st_distance(rp_3rooms.sf, 
              supermarkets.sf[nearest,], 
              by_element=TRUE) )
```

## Number of ... within given distance

The first step is to create buffers for each HDB flat. We create two buffers, one of 350m and one of 1000m (1km).

```{r}
#| eval: false
buffer_350  = st_buffer(rp_3rooms.sf, 350)
buffer_1000 = st_buffer(rp_3rooms.sf, 1000)
```

Using these buffers will allow us to compute the number of a given variable inside the buffer.

### Numbers of kindergartens within 350m

```{r}
#| eval: false
kindergartens.sf = retrieved_variables.sf %>%
  filter(variable_name == "kindergartens")
```

```{r}
#| eval: false
rp_3rooms.sf$nb_of_kindergartens = lengths(
  st_intersects(buffer_350, kindergartens.sf))
```

### Numbers of childcare centres within 350m

```{r}
#| eval: false
child_care.sf = retrieved_variables.sf %>%
  filter(variable_name == "child_care")
```

```{r}
#| eval: false
rp_3rooms.sf$nb_of_childcare = lengths(
  st_intersects(buffer_350, child_care.sf))
```

### Numbers of bus stop within 350m

```{r}
#| eval: false
rp_3rooms.sf$nb_of_bus_stops = lengths(
  st_intersects(buffer_350, bus_stops.sf))
```

### Numbers of primary school within 1km

```{r}
#| eval: false
rp_3rooms.sf$nb_of_primary_schools = lengths(
  st_intersects(buffer_1000, primary_schools.sf))
```

## Saving the data

We will save the data into a rds file to avoid running all the previous code every time we would like to work on the assignment since some of these steps are very lengthy.

```{r}
#| eval: false
write_rds(rp_3rooms.sf, "data/geospatial/final_dataset.rds")
```

```{r}
rp_3rooms.sf = read_rds("data/geospatial/final_dataset.rds")
```

## Check for missing data

```{r}
sum(is.na(rp_3rooms.sf))
```

There is no missing that so we can move on to creating the hedonistic pricing models.

# Hedonic Pricing model

The first step to our hedonistic pricing model is to clean the name of our fields. We do so using the rename function.

```{r}
rp_3rooms.sf = rp_3rooms.sf %>%
  rename("01_to_03" = "01 TO 03", "04_to_06" = "04 TO 06", "07_to_09" = "07 TO 09",
         "10_to_12" = "10 TO 12", "13_to_15" = "13 TO 15", "16_to_18" = "16 TO 18",
         "19_to_21" = "19 TO 21", "22_to_24" = "22 TO 24", "25_to_27" = "25 TO 27",
         "28_to_30" = "28 TO 30", "31_to_33" = "31 TO 33", "34_to_36" = "34 TO 36",
         "37_to_39" = "37 TO 39", "40_to_42" = "40 TO 42", "43_to_45" = "43 TO 45",
         "46_to_48" = "46 TO 48", "New_Generation" = "New Generation", 
         "Model_A" = "Model A", "Premium_Apartment" = "Premium Apartment")
```

## Splitting the data

Now that are fields are cleaned, we should split the data into two data frames. One data frame used for training and one for testing. We do so using the filter and grepl functions.

Note that you can use regular expressions to be more efficient when trying to string match.

```{r}
rp_analysis.sf = rp_3rooms.sf %>%
  filter(grepl("202[12]", month))

rp_testing.sf = rp_3rooms.sf %>%
  filter(grepl("2023", month))
```

# Conventional OLS method

The first analysis we will perform is the "standard" or "conventional" OLS. This is a multiple linear regression that takes into account only our independent variables. It does not use geographical location as a determinant of the resale price of an HDB flat.

Before running the analysis, there are a few preliminary steps. We should check for: non-linear relationships between the independent variables and the dependent variables; multicollinearity; heteroscedasticity.

## Multicollinearity

Using the corrplot package and its corrplot function, we get a clean correlation matrix that indicates positive correlation with the color blue and negative with red.

```{r}
cor_test = cor(rp_3rooms.sf %>% 
  st_drop_geometry() %>% 
  select(-c(1, 22, 23)))

corrplot::corrplot(cor_test, diag = FALSE, order = "AOE", tl.pos = "td", 
                   tl.cex = 0.5, method = "number", type = "upper",
                   number.cex = 0.3)
```

We find that there is a perfect correlation between the lease_commence_date and remaining_lease variables, so we will include only one of the two in our OLS regression.

## MLR model

To test for linearity and heteroscedasticity, we first need to run the regression. Note that using the next code chunk, you can retrieve the list of variables in our data frame.

```{r}
#| eval: false
colnames(rp_3rooms.sf)
```

```{r}
#| warning: false
rp_analysis.mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease +
                      `01_to_03` + `04_to_06` + `07_to_09` + `10_to_12` + `13_to_15` + 
                      `16_to_18` + `19_to_21` + `22_to_24` + `25_to_27` + `28_to_30` + 
                      `31_to_33` + `34_to_36` + `37_to_39` + `40_to_42` + `43_to_45` + 
                      `New_Generation` + Improved + `Model_A` + Simplified + Standard + 
                      `Premium_Apartment` + DBSS + distance_to_eldercare + 
                      distance_to_food + distance_to_mrt + distance_to_park + 
                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                    data = rp_analysis.sf)

stargazer(rp_analysis.mlr, type="text")
```

It seems like our regression is statistically significant; the F-statistic is very high which indicates that the model is globally significant at a 1% significance level. Looking at the significance of our variables, we realize that most of them are significant apart from the following dummy variables: 43_to_45, 40_to_42, 37_to_39, 34_to_36, and 31_to_33. We may want to drop these variables in the future. Now, let's look at the R-squared of this model. We find that the chosen variables explain about 64% of the total variation in the resale price. It is somewhat a good fit but i think we can improve that with a geographically weighted model.

## Linearity

To check for linearity, we can plot the residuals agaisnt the fitted values of the regression. If we observe a random cloud of points, it means that there are no non-linearity issues.

```{r}
plot(residuals(rp_analysis.mlr), fitted.values(rp_analysis.mlr),
     xlab="Residuals", ylab="Fitted Values")
```

Looking at the above scatter plot, it seems that the cloud is not so random, meaning that it is likely that we observe non-linear relationships between some independent variables and the dependent variable.

## Heteroscedasticity

To check if there is heteroscedasticity (we want homoscedasticity), we will use the Goldfeld-Quandt test.

```{r}
gqtest(rp_analysis.mlr)
```

The p-value proves that there is homoscedasticity of the residuals. So, we can consider our previous findings to be accurate under the Gauss-Markov theorem.

# GWR methods

We will now moce on to our first geographically weighted regression. We will be using the gwr.basic function of the *GWmodel* package. We shall use an adaptive bandwidth for our model.

## Converting sf data frame to SpatialPointDataFrame

The first step to what I described above is to convert our sf data frame into a SpatialPoint data frame.

```{r}
#| eval: false
rp_analysis.sp = as_Spatial(rp_analysis.sf)
```

## Computing adaptive bandwidth

Once this is done, we use the bw.gwr function to compute the bandwidth that we should use in the regression.

```{r}
#| eval: false
bw_analysis_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + remaining_lease +
                      X01_to_03 + X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + 
                      X16_to_18 + X19_to_21 + X22_to_24 + X25_to_27 + X28_to_30 + 
                      X31_to_33 + X34_to_36 + X37_to_39 + X40_to_42 + X43_to_45 + 
                      New_Generation + Improved + Model_A + Simplified + Standard + 
                      Premium_Apartment + DBSS + distance_to_eldercare + 
                      distance_to_food + distance_to_mrt + distance_to_park + 
                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                      data = rp_analysis.sp, approach="CV", kernel="gaussian",
                      adaptive=TRUE, longlat=FALSE)
```

```{r}
#| eval: false
write_rds(bw_analysis_adaptive, "data/model/bw_analysis_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/model/bw_analysis_adaptive.rds")
bw_adaptive
```

It looks like we should use a bandwidth of 1357 neighbor points.

## Constructing the adaptive bandwidth gwr model

Now, we can use the gwr.basic function to obtain our regression.

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + remaining_lease + X01_to_03 +
                            X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + X16_to_18 + X19_to_21 +
                            X22_to_24 + X25_to_27 + X28_to_30 + X31_to_33 + X34_to_36 + X37_to_39 + 
                            X40_to_42 + X43_to_45 + New_Generation + Improved + Model_A + Simplified + 
                            Standard + Premium_Apartment + DBSS + distance_to_eldercare + distance_to_food + 
                            distance_to_mrt + distance_to_park + distance_to_mall + distance_to_supermarkets + 
                            nb_of_kindergartens + nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                          data = rp_analysis.sp, bw=bw_adaptive, kernel = 'gaussian', adaptive=TRUE, 
                          longlat = FALSE)
```

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/model/gwr_analysis_adaptive.rds")
```

```{r}
gwr_adaptive = read_rds("data/model/gwr_analysis_adaptive.rds")
gwr_adaptive
```

Looking at the results of the Geographically Weighted Regression, we obtain much better results as the R-squared is now almost 20% higher, meaning that adding the geographic weights has had a positive impact on the quality of the model.

# Preparing coordinates data

Before moving on our last two regression models based around Random Forest, we shall prepare so additional data necessary to properly use the coming functions.

## Extracting coordinates data

Using the st_coordinates function of the sf package, we extract the coordinates from the POINT geometries and store them in a rds file for later use.

```{r}
#| eval: false
coords_train <- st_coordinates(rp_analysis.sf)
coords_test <- st_coordinates(rp_testing.sf)
```

```{r}
#| eval: false
write_rds(coords_train, "data/model/coords_train.rds" )
write_rds(coords_test, "data/model/coords_test.rds" )
```

```{r}
coords_train = read_rds("data/model/coords_train.rds")
coords_test = read_rds("data/model/coords_test.rds")
```

## Dropping geometry

We shall also create new data frames that do not store the sf geometry. Using the st_drop_geometry function, we remove the geometry column and we pipe the select function to remove unnecessary columns.

```{r}
rp_analysis.no_geom = rp_analysis.sf %>% 
  st_drop_geometry() %>%
  select(-c(1, 3, 22:23))

rp_testing.no_geom = rp_testing.sf %>% 
  st_drop_geometry() %>%
  select(-c(1, 3, 22:23))
```

# Random Forest Model

We will now address how to calibrate the Random Forest model and build predictions with it.

## Calibrating the Random Forest model

Before calibrating the model, we have to once again clean the column names. To do so, we use the clean_names function of the janitor package.

```{r}
rp_analysis.no_geom = rp_analysis.no_geom %>%
  clean_names()

rp_testing.no_geom = rp_testing.no_geom %>%
  clean_names()
```

We can now use the ranger function of the ranger package to generate the Random Forest model.

```{r}
set.seed(2410)
rf <- ranger(resale_price ~ floor_area_sqm + remaining_lease +
               x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + 
               x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + 
               x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + 
               new_generation + improved + model_a + simplified + standard + 
               premium_apartment + dbss + distance_to_eldercare + 
               distance_to_food + distance_to_mrt + distance_to_park + 
               distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
               nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
             data = rp_analysis.no_geom)
```

```{r}
rf
```

It looks like this is our best model so far and has great explanatory power. We obtain a 89% R-squared value, which by far is an improvement from our previous models.

## Predicting by using test data

### Preparing the test data

The code chunk below will be used to combine the test data with its corresponding coordinates data.

```{r}
test_data = cbind(rp_testing.no_geom, coords_test)
```

### Predicting with test data

Next, we will use the predict function of the ranger package to predict the resale value of HDB flats using the testing data and the rf model previously calibrated.

```{r}
test_pred = predict(rf, test_data, 
                    x.var.name="X", y.var.name="Y", 
                    local.w=1, global.w=0)
```

### Converting the predicting output into a data frame

The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualization and analysis.

```{r}
test_pred.df = as.data.frame(test_pred)
```

In the code chunk below, cbind() is used to append the predicted values onto test_data

```{r}
test_data_p <- cbind(test_data, test_pred.df)
```

### Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$prediction)
```

We should use this value and compare it with the one of the geographical random forest model.

### Visualising the predicted values

Alternatively, scatter plot can be used to visualize the actual resale price and the predicted resale price by using the code chunk below.

```{r}
ggplot(data = test_data_p,
       aes(x = prediction,
           y = resale_price)) +
  geom_point()
```

It looks like we have a great model since the plot shows an almost perfect linear relationship.

# Geographical Random Forest Model

Now, we can move on to our last regression model, the Geographical Random Forest model. I provide you with the code necessary for this whole section of calibrating the model and making predictions. Unfortunately, my computer does not allow me to run the prediction as when I load the model into my environment, my computer crashes.

In this section, we use the grf function of the SpatialML package with an adaptive kernel and the previously computed adaptive bandwidth.

## Calibrating the model using training data

```{r}
#| eval: false
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + remaining_lease +
                       x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + 
                       x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + 
                       x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + 
                       new_generation + improved + model_a + simplified + standard + 
                       premium_apartment + dbss + distance_to_eldercare + 
                       distance_to_food + distance_to_mrt + distance_to_park +
                       distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                       nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools,
                     dframe=rp_analysis.no_geom, bw=bw_adaptive, kernel="adaptive",
                     coords=coords_train)
```

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

## Predicting by using test data

### Preparing the test data

The code chunk below will be used to combine the test data with its corresponding coordinates data.

```{r}
#| eval: false
test_data = cbind(gwRF_adaptive, coords_test)
```

### Predicting with test data

Next, we will use the predict function of the ranger package to predict the resale value of HDB flats using the testing data and the rf model previously calibrated.

```{r}
#| eval: false
test_pred = predict(rf, test_data, 
                    x.var.name="X", y.var.name="Y", 
                    local.w=1, global.w=0)
```

### Converting the predicting output into a data frame

The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualization and analysis.

```{r}
#| eval: false
test_pred.df = as.data.frame(test_pred)
```

In the code chunk below, cbind() is used to append the predicted values onto test_data

```{r}
#| eval: false
test_data_p <- cbind(test_data, test_pred.df)
```

### Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
#| eval: false
rmse(test_data_p$resale_price, 
     test_data_p$prediction)
```

### Visualising the predicted values

Alternatively, scatter plot can be used to visualize the actual resale price and the predicted resale price by using the code chunk below.

```{r}
#| eval: false
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# Conclusion

If we have to compare the models, it seems like the random forest model is the better model with the highest explanatory power. I can only imagine that the geographically weighted random forest performs even better with great predictive ability. Models like a conventional OLS are not relevant anymore in a world where geography has such an importance in the prediction of HDB flats prices.
