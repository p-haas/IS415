---
title: "Predicting HDB Public Housing Resale Prices using Geographically Weighted Methods"
subtitle: "Take-home Exercise 3"

author: "Pierre HAAS"

date: "March 07, 2023"
date-modified: "`r Sys.Date()`"

execute:
  eval: true
  echo: true
  warning: false
editor: visual

number-sections: true
---

# Getting Started

In this Take-home Exercise 3, we will try using geographically weighted methods to predict the resale price of HDB public housing in Singapore. You will find in the next to sections a brief description of the data sets used and where to retrieve them as well as the list of all the libraries needed for this project and a description of their specific usage.

## Installing and Loading Packages

For the purpose our this project, we have selected a list of libraries that will allow to perform all the necessary data cleaning, handling and analysis.

The following list shows the libraries that will be used and their purpose:

To install and load the packages, we use the p_load() function of the pacman package. It automatically checks if packages are installed, installs them if they are not installed, and loads the packages into the R environment.

```{r}
#| eval: false
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep,
               GWmodel, tmap, tidyverse, gtsummary,
               ranger, SpatialML, rsample, Metrics,
               janitor)
```

```{r}
#| eval: false
pacman::p_load(knitr, httr, onemapsgapi, xml2, rvest,
               stringr, ggmap)
```

```{r}
#| eval: false
#| echo: false
library(tidygeocoder)
```

```{r}
options(digits = 15)
```

## The Data

mp subzone

hdb data

# Retrieving and Importing the Geospatial Data

In this section, we will be importing the data frames into our R environment and performing basic data wrangling, more on that to come as we load the data sets.

## Master Plan 2019 with sub-zones

```{r}
#| eval: false
mpsz.sf = st_read(dsn = "data/geospatial/mpsz2019",
                  layer = "MPSZ-2019") %>%
  st_transform(crs=3414) %>%
  st_make_valid()
```

## MRT Train stations

```{r}
#| eval: false
mrt_exits.sf = st_read(dsn = "data/geospatial/TrainStationExit/TrainStationExit",
                       layer = "Train_Station_Exit_Layer") %>%
  st_transform(crs=3414)
```

## Bus Stop Locations

```{r}
#| eval: false
bus_stops.sf = st_read(dsn = "data/geospatial/BusStopLocation/BusStop_Feb2023",
                       layer = "BusStop") %>%
  st_transform(3414)
```

## Supermarket data

```{r}
#| eval: false
supermarkets.sf = st_read("data/geospatial/supermarkets/supermarkets-kml.kml") %>%
  st_transform(3414)
```

## Retrieving the independent variables using the OneMap API

We will be using the One Map Singapore API to retrieve some of the independent variables that we will be using later on in our regression model.

We will be taking a few preliminary steps before extracting the variables. You will find everything you need to replicate my work in the following sections.

### Retrieving the API token using the *onemapsgapi* package

The first step to getting the API key is to create an account on the One Map website (click [here](https://www.onemap.gov.sg/docs/#register-free-account) to access the registration tutorial).

Now that you are registered, we may retrieve the API key using the *onemapsgapi* package and its get_token() function. By simply entering your registration email and password, you will get a valid API key for a three-day period.

Note that if you want to learn more about the One Map API, you should definitely check their [website](https://www.onemap.gov.sg/docs/) and their [web app](https://app.swaggerhub.com/apis/onemap-sg/new-onemap-api/1.0.4) that allows to test the API's services.

```{r}
#| eval: false
token_api = get_token(email = "xxx@xxx.xxx", 
                      password = "...")
```

### Using the API to retrieve the variables available on OneMap

The first step to retrieving our independent variables is to look at the available data provided by One Map. We do so using the httr package that allows us to make HTML queries. Using the GET() function, we retrieve every theme info of the OneMap API.

```{r}
#| eval: false
url = "https://developers.onemap.sg/privateapi/themesvc/getAllThemesInfo"

query <- list("token" = token_api)
result <- GET(url, query = query)
```

Using the content() function, we may look at the information retrieved, however, you should be made aware that the result from our GET() command is quite messy. So, you will see in the next section how we extract the theme names and store them into a list.

```{r}
#| eval: false
content(result)
```

### Creating the variable list

As explained, the information retrieved is quite messy. So, we will be storing in a list the essential information that we will use later on.

Using a for loop, I store the theme specific information into a list called themes_list.

```{r}
#| eval: false
themes_list = list()

for (i in 1:length(content(result)$Theme_Names)){
  themes_list = append(themes_list, content(result)$Theme_Names[[i]])
}
```

To take a look at the information stored into our list, you may use the next code chunk. However, since the list is quite long, I won't print the output/information here and will directly give you a list you the variables I have extracted.

```{r}
#| eval: false
themes_list
```

In the next code chunk, you may see the variables that I carefully selected. They give information about: Eldercare Services, Hawker Centres, Food Courts, Parks in Singapore, Kindergartens, and Childcare Centres.

```{r}
#| eval: false
ind_variables = c("eldercare", "hawkercentre", "hawkercentre_new", "healthier_hawker_centres", "maxwell_fnb_map", "healthiercaterers", "relaxsg", "nationalparks", "kindergartens", "childcare")
```

I have also picked-up some other interesting variables that may be included later in my regression model to improve the quality of the future model.

```{r}
#| eval: false
extra_variables = c("lta_cycling_paths", "disability", "hsgb_fcc", "sportsg_sport_facilities", "ssc_sports_facilities", "wastedisposalsite", "drainageconstruction", "cpe_pei_premises", "sewerageconstruction", "danger_areas", "aquaticsg", "moh_isp_clinics", "heritagetrees", "nparks_activity_area",  "exercisefacilities", "mha_uav_2015", "playsg", "underconstruction_facilities", "preschools_location", "hdbcyclingunderconstruction",  "boundary_5km",  "hdbluppropunderconstruction", "parkingstandardszone", "libraries", "cyclingpath", "dengue_cluster", "greenbuilding", "nparks_uc_parks", )
```

### Retrieving the available variables with the OneMap API

Using the code chunk below, you can retrieve the location name and its coordinates based on the previously created list of variables. We store this information into a data frame to make our use of this data easier

```{r}
#| eval: false
df = data.frame()

url = "https://developers.onemap.sg/privateapi/themesvc/retrieveTheme"

for (x in ind_variables){
  
  query <- list("queryName" = x, "token" = token_api)
  
  result <- GET(url, query = query)
  
  print("Start")
  print(x)
  
  for (i in 2:length(content(result)$SrchResults)){
    new_row = c(content(result)$SrchResults[[i]]$NAME,
                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, ",", 2)[1],
                str_split_fixed(content(result)$SrchResults[[i]]$LatLng, ",", 2)[2],
                content(result)$SrchResults[[1]]$Theme_Name)
    
    df = rbind(df, new_row)
  }

}

colnames(df)<-c("location_name", "lat", "lon", "variable_name")
```

Note that retrieving the data takes a few minutes, so I took care of exporting the data frame into a csv file to avoid running the previous code chunk multiple times.

```{r}
#| eval: false
write_csv(df, "data/geospatial/retrieved_variables.csv")
```

We now have part of our independent variables stored into a csv file that we will import back into our R environment.

```{r}
#| eval: false
retrieved_variables = read_csv("data/geospatial/retrieved_variables.csv", )
```

If we look back to the variables we chose to retrieve using the One Map API, we had sub-categories to our independent variables.

```{r}
#| eval: false
unique(retrieved_variables$variable_name)
```

For instance, we have four sub-categories for hawker centres and food courts. We shall simplify our work by renaming these sub-categories under one name only. Using the next code chunk, we may do so.

```{r}
#| eval: false
retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Eldercare Services", 
                                                "eldercare")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "^Hawker Centres$", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Hawker Centres_New", 
                                                "hawker_centres")


retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Healthier Hawker Centres", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Maxwell Chambers F&B map", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Healthier Caterers", 
                                                "hawker_centres")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Parks@SG", 
                                                "parks")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Parks", 
                                                "parks")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Kindergartens", 
                                                "kindergartens")

retrieved_variables$variable_name = str_replace(retrieved_variables$variable_name, 
                                                "Child Care Services", 
                                                "child_care")
```

```{r}
#| eval: false
unique(retrieved_variables$variable_name)
```

We shall now proceed to creating sf objects from the extracted coordinates.

### Transforming the data frame to sf data frame

Using the st_as_sf() function from the *sf* package, we can transform our set of coordinates into sf point objects.

```{r}
#| eval: false
retrieved_variables.sf = st_as_sf(retrieved_variables,
                                  coords = c("lon", "lat"),
                                  crs = 4326, remove = FALSE) %>%
  st_transform(3414)
```

We shall check if our data points have been correctly transformed by plotting the points on the Singapore map using the *tmap* package.

```{r}
#| eval: false
tmap_mode("plot")
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(retrieved_variables.sf)+
  tm_dots()
```

# Retrieving and Importing the Aspatial Data

## Resale Price of HDB flats

After retrieving the resale HDB prices from the data.gov.sg website, I moved the folder into my data folder, it contains all the data sets necessary for our analysis, and more precisely stored it into the aspatial folder.

The downloaded data is stored in a folder called resale-flat-prices, itself containing multiple files. To choose the right file, we can use the base r function list.files() to return a list of the file's names.

```{r}
#| eval: false
path = "data/aspatial/resale-flat-prices"
files = list.files(path)
files
```

Looking at the above list, it seems clear that we should use file n°5 as it contains the data from 2017 and later. It gives us the intuition that we may need to treat the data and take a subset of data rows within the analysis period (2021 to 2023). We will import the data using the read_csv() function from the readr package.

```{r}
#| eval: false
resale_price = read_csv(file = paste(path, files[5], sep = "/"))
```

We should take a quick look at the fields contained in the resale_price data frame. We do so using the glimpse() function.

```{r}
#| eval: false
glimpse(resale_price)
```

The available fields are as follows:

-   month: it indicates the month of transaction
-   town: it indicates the district
-   flat_type: it indicates the number of rooms in the flat
-   block: it indicates the block number of the HDB flat
-   street_name: it indicates the street number and name
-   storey_range: it indicates the floor number of the flat
-   floor_area_sqm: it indicates the floor area of the flat in square meter
-   flat_model: it indicates the HDB flat model
-   lease_commence_date: it indicates the starting year date of the lease (in Singapore, HDB flats are 99-year leaseholds)
-   remaining_lease: it indicates the remaining years and months on the 99-year leasehold
-   resale_price: it indicates the resale price of a given flat in SGD (Singapore dollars)

Now, we will use the head() function to look at the available data in each field.

```{r}
#| eval: false
head(resale_price)
```

### Selecting trasactions of 3 Room apartments from 2021 onwards

My previously mentioned intuition is confirmed. The month variable takes data starting in January 2017 and, through quick data manipulation, we will have to restrain the data frames to the 2021-2022 analysis period and 2023 data testing period. In addition, it looks like the town field in unnecessary and may be dropped. We only need the block and steet_name fields to geocode the addresses and create sf geometry.

```{r}
#| eval: false
rp_3rooms = resale_price %>%
  select(-2) %>%
  filter(grepl("202[123]", month)) %>%
  filter(flat_type == "3 ROOM")
```

### Transforming the storey_range column to dummy variables

Since we would like to include the storey_range variable in our analysis and the column only takes categorical data, we shall create dummy variables to indicate the storey range of each HDB flat.

The first step is to look at what are the unique values in the column. Using the unique() function, we retrieve in the form of a list the unique categorical values.

```{r}
#| eval: false
unique(rp_3rooms$storey_range)
```

Now, using a for loop and the ifelse() function, we will create a new column for each unique categorical variable contained in the storey_range field and assign a 1 if the particular HDB flat belongs to the specific storey range.

```{r}
#| eval: false
for (i in unique(rp_3rooms$storey_range)){
  rp_3rooms[i] = ifelse(rp_3rooms$storey_range == i, 1, 0)
}
```

### Transforming the flat_model column to dummy variables

```{r}
#| eval: false
unique(rp_3rooms$flat_model)
```

```{r}
#| eval: false
for (i in unique(rp_3rooms$flat_model)){
  rp_3rooms[i] = ifelse(rp_3rooms$flat_model == i, 1, 0)
}
```

### Transforming the remaining_lease to a numerical variable

```{r}
#| eval: false
lease_remaining = list()

for (i in 1:nrow(rp_3rooms)){
  
  lease = str_extract_all(rp_3rooms$remaining_lease, "[0-9]+")[[i]]
  
  year = as.numeric(lease[1])
  
  if (length(lease) < 2){
    
    lease_remaining = append(lease_remaining, year)
  
    } else {
      
    month = as.numeric(lease[2])
    
    lease_remaining = append(lease_remaining, round(year+month/12, 2))
  
    }

}
```

```{r}
#| eval: false
rp_3rooms$remaining_lease = as.numeric(lease_remaining)
```

### Geocoding the address

The first step to geocoding the address - retrieving the latitude and longitude based in the address - is to create a new column that stores the cleaned full address. Consequently, we create a new field called cleaned_address that combines both the block and street_name fields. This will allow us to retrieve the geocode of the HDB flats in our query.

```{r}
#| eval: false
rp_3rooms["cleaned_address"] = paste(rp_3rooms$block, rp_3rooms$street_name, sep = " ")
```

```{r}
#| eval: false
url = "https://developers.onemap.sg/commonapi/search"

full_address = list()
latitude = list()
longitude = list()

for (address in rp_3rooms$cleaned_address){
  query <- list("searchVal" = address,
                "returnGeom" = "Y", "getAddrDetails" = "Y")
  res <- GET(url, query = query)
  
  full_address = append(full_address, content(res)$results[[1]]$ADDRESS)
  latitude = append(latitude, content(res)$results[[1]]$LATITUDE)
  longitude = append(longitude, content(res)$results[[1]]$LONGITUDE)
}

```

```{r}
#| eval: false
rp_3rooms$full_address = full_address
rp_3rooms$lat = latitude
rp_3rooms$lon = longitude
```

```{r}
#| eval: false
options(digits=15)

rp_3rooms$full_address = apply(rp_3rooms[, 28], 2, as.character)
rp_3rooms$lat = apply(rp_3rooms[, 29], 2, as.numeric)
rp_3rooms$lon = apply(rp_3rooms[, 30], 2, as.numeric)
```

```{r}
#| eval: false
write.csv(rp_3rooms, "data/aspatial/geocoded_resale_price.csv", row.names = FALSE)
```

We import the geocoded data back into the R environment.

```{r}
#| eval: false
rp_3rooms = read_csv("data/aspatial/geocoded_resale_price.csv")
```

### Dropping the unrelevant variables

```{r}
#| eval: false
rp_3rooms.cleaned = rp_3rooms %>%
  select(-c("flat_type", "block", "street_name", "storey_range", "flat_model", "cleaned_address", "full_address"))
```

### Creating the sf geometry

```{r}
#| eval: false
rp_3rooms.sf = st_as_sf(rp_3rooms.cleaned,
                        coords = c("lon", "lat"),
                        crs = 4326, remove = FALSE) %>%
  st_transform(3414)
```

```{r}
#| eval: false
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(rp_3rooms.sf)+
  tm_dots()
```

## Primary School data

### Scrapping Primary Schools online with *rvest* package

```{r}
#| eval: false
url = "https://sgschooling.com/school/"

res = GET(url)

wiki_read <- xml2::read_html(res, encoding = "UTF-8")

testt = wiki_read %>%
  html_elements("a")

testt1 = testt[grepl("school/", testt)]

pattern <- ">(.*)<"
result <- regmatches(testt1, regexec(pattern, testt1))

schools = list()

for (i in 1:length(result)){
  new_row = result[[i]][2]
  schools = append(schools, new_row)
}

```

### Using the OneMap API to retrieve the schools' coordinates

```{r}
#| eval: false
primary_sc = data.frame()

url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:length(schools)){
  
  query <- list("searchVal" = schools[[i]],
                "returnGeom" = "Y", 
                "getAddrDetails" = "Y")
  result <- GET(url, query = query)
  
  if (length(content(result)$results) == 0){
    new_row = c(schools[[i]], 
              NA,
              NA,
              NA)
    primary_sc = rbind(primary_sc, new_row)
    
  } else{
    new_row = c(schools[[i]], 
                content(result)$results[[1]]$ADDRESS,
                content(result)$results[[1]]$LATITUDE,
                content(result)$results[[1]]$LONGITUDE)
    
    primary_sc = rbind(primary_sc, new_row)
    
  }

}

colnames(primary_sc) = c("location_name", "full_address", "lat", "lon")
```

### Checking for missing information

```{r}
#| eval: false
sum(is.na(primary_sc$full_address))
```

It seems like we have missing data for 9 primary schools. It is probably because the name of the school is not recognized by the API, so we will try to clean them to get their information into our data frame.

```{r}
#| eval: false
primary_sc %>%
  filter(is.na(primary_sc$full_address) == TRUE) %>%
  select(1)
```

By checking on the web, it seems like the "Juying Primary School" has merged with the "Pioneer Primary School", so we may drop the "Juying Primary School" from the data frame.

```{r}
#| eval: false
primary_sc1 = primary_sc %>%
  filter(!grepl("Juying Primary School", location_name))
```

Now, regarding the remaining schools, after performing a tests, it seems like I am not able to retrieve the information using the API so we will be filling the values with, first, the Google Maps API and then, if there are still some empty information, I will be filling the values manually by looking online.

You may want to check the next two code chunks I have used to try and retrieve the missing information using the OneMap API.

```{r}
#| eval: false
primary_sc1$location_name = str_replace(primary_sc1$location_name, 
                                        "St. ", "")

# I have also tried replacing the string with: "Saint" and "St" but I am not able to make the API retrieve the data
```

```{r}
#| eval: false
url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:nrow(primary_sc1)){
  
  if (is.na(primary_sc1[i, 2])){
    query <- list("searchVal" = primary_sc1[i, 1],
                  "returnGeom" = "Y", 
                  "getAddrDetails" = "Y")
    result <- GET(url, query = query)
    
    if (length(content(result)$results) == 0){
      next
      
      } else{
        primary_sc1[i, 2] = content(result)$results[[1]]$ADDRESS
        primary_sc1[i, 3] = content(result)$results[[1]]$LATITUDE
        primary_sc1[i, 4] = content(result)$results[[1]]$LONGITUDE
    
  }} else{ next
  
  }
}
```

These are the schools with the missing information.

```{r}
#| eval: false
primary_sc1 %>%
  filter(is.na(primary_sc1$full_address) == TRUE) %>%
  select(1)
```

Using the Google Maps API and its library *ggmap*, we will loop through the schools with missing coordinates. Since all these schools are located consecutively in the data frame, I use the following structure to create the list of schools.

While looping through these schools, we use the geocode() function of *ggmap* to retrieve the latitude and longitude based on the school name and assign these values in the corresponding missing cells of the primary_sc1 data frame.

```{r}
#| eval: false
register_google(key = "xxx")

missing_sc = primary_sc1[grep("St. Andrew’s Junior School", primary_sc1$location_name):grep("St. Stephen’s School", primary_sc1$location_name), 1]

for (i in missing_sc){
  coords = geocode(i)
  primary_sc1[grep(i, primary_sc1$location_name), 3] = coords$lat
  primary_sc1[grep(i, primary_sc1$location_name), 4] = coords$lon
  
}
```

We should check if we still have missing coordinates. Note that we are missing information in the full_address column, however, it is not a great deal since we will be using the coordinates to create the sf objects later on.

```{r}
#| eval: false
primary_sc1 %>%
  filter(is.na(primary_sc1$lat) == TRUE)
```

We are still missing coordinates for three schools. We will be filling these values manually.

```{r}
#| eval: false
primary_sc1[grep("St. Anthony’s Primary School", primary_sc1$location_name), 3:4] = list(1.3796337949311097, 103.75033229288428)

primary_sc1[grep("St. Gabriel’s Primary School", primary_sc1$location_name), 3:4] = list(1.3499742816603595, 103.86268328706147)

primary_sc1[grep("St. Stephen’s School", primary_sc1$location_name), 3:4] = list(1.3189794629543288, 103.9179118196036)
```

We are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).

```{r}
#| eval: false
write.csv(primary_sc1, "data/aspatial/primary_schools.csv", row.names = FALSE)
```

```{r}
#| eval: false
primary_schools = read_csv("data/aspatial/primary_schools.csv")
```

### Creating the sf data frame

```{r}
#| eval: false
primary_schools.sf = st_as_sf(primary_schools,
                         coords = c("lon", "lat"),
                         crs = 4326, remove = FALSE) %>%
  st_transform(3414)
```

## Shopping Mall data

### Scraping Wikipedia to extract the list of malls in Singapore

```{r}
#| eval: false
url = "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"

res = GET(url)

wiki_read <- xml2::read_html(res, encoding = "UTF-8")

testt = wiki_read %>%
  html_nodes("div.div-col") %>%
  html_elements("ul") %>%
  html_elements("li") %>%
  html_text()

malls = str_replace(testt, "\\[.*]", "")

```

### Using the OneMap API to retrieve the malls' coordinates

```{r}
#| eval: false
shp_malls = data.frame()

url = "https://developers.onemap.sg/commonapi/search"

for (i in 1:length(malls)){
  
  query <- list("searchVal" = malls[i],
                "returnGeom" = "Y", 
                "getAddrDetails" = "Y")
  result <- GET(url, query = query)
  
  if (length(content(result)$results) == 0){
    new_row = c(malls[i], 
              NA,
              NA,
              NA)
    shp_malls = rbind(shp_malls, new_row)
    
  } else{
    new_row = c(malls[i], 
                content(result)$results[[1]]$ADDRESS,
                content(result)$results[[1]]$LATITUDE,
                content(result)$results[[1]]$LONGITUDE)
    
    shp_malls = rbind(shp_malls, new_row)
    
  }

}

colnames(shp_malls) = c("location_name", "full_address", "lat", "lon")
```

```{r}
#| eval: false
sum(is.na(shp_malls$full_address))
```

### Filling missing information with ggmap

```{r}
#| eval: false
shp_malls %>%
  filter(is.na(shp_malls$full_address) == TRUE) %>%
  select(1)
```

Before using the Google Maps API, I decided to research about these nine malls and found that:

-   The PoMo mall is now called GR.iD, so we shall change the name in the data frame

-   The KINEX mall is found under the name KINEX, so we shall remove the additional information from the name

-   The Paya Lebar Quarter mall is found under the name PLQ mall on Google Maps, so we shall change the name in the data frame

```{r}
#| eval: false
shp_malls[grep("PoMo", shp_malls$location_name), 1] = "GR.iD";

shp_malls[grep("KINEX", shp_malls$location_name), 1] = "KINEX";

shp_malls[grep("PLQ", shp_malls$location_name), 1] = "PLQ mall"
```

```{r}
#| eval: false
register_google(key = "xxx")

missing_malls = shp_malls %>%
  filter(is.na(shp_malls$full_address) == TRUE) %>%
  `$`(location_name)

for (i in list(missing_malls)[[1]]){
  coords = geocode(i)
  shp_malls[grep(i, shp_malls$location_name), 3] = coords$lat
  shp_malls[grep(i, shp_malls$location_name), 4] = coords$lon
  
}
```

```{r}
#| eval: false
shp_malls %>%
  filter(is.na(shp_malls$lat) == TRUE)
```

### Filling missing information manually

We are still missing coordinates of the KINEX mall. We will be filling these values manually.

```{r}
#| eval: false
shp_malls[grep("KINEX", shp_malls$location_name), 3:4] = c(1.314893715213727, 103.89480904154526)
```

We are now good to go, we can proceed to creating a sf data frame, however, before doing so, I will export the data frame into my data folder to avoid using the Google Maps API again (it is free under a limited amount of queries).

```{r}
#| eval: false
write.csv(shp_malls, "data/aspatial/shopping_malls.csv", row.names = FALSE)
```

```{r}
#| eval: false
shp_malls = read_csv("data/aspatial/shopping_malls.csv")
```

```{r}
#| eval: false
shp_malls.sf = st_as_sf(shp_malls,
                        coords = c("lon", "lat"),
                        crs = 4326, remove = FALSE) %>%
  st_transform(3414)
```

# Preparing the final data frame

In this section, we will prepare the following variables:

Proxomity to CBD Proximity to eldercare Proximity to foodcourt/hawker centres Proximity to MRT Proximity to park Proximity to good primary school Proximity to shopping mall Proximity to supermarket Numbers of kindergartens within 350m Numbers of childcare centres within 350m Numbers of bus stop within 350m Numbers of primary school within 1km

## Proximity variables

### Proximity to eldercare

```{r}
#| eval: false
eldercare.sf = retrieved_variables.sf %>%
  filter(variable_name == "eldercare")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, eldercare.sf)

rp_3rooms.sf$distance_to_eldercare = as.numeric(
  st_distance(rp_3rooms.sf, 
              eldercare.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to food court / hawker centres

```{r}
#| eval: false
hawker_centres.sf = retrieved_variables.sf %>%
  filter(variable_name == "hawker_centres")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, hawker_centres.sf)

rp_3rooms.sf$distance_to_food = as.numeric(
  st_distance(rp_3rooms.sf, 
              hawker_centres.sf[nearest,], 
              by_element=TRUE) )
```

### Proximity to MRT

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, mrt_exits.sf)

rp_3rooms.sf$distance_to_mrt = as.numeric(
  st_distance(rp_3rooms.sf, 
              mrt_exits.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to park

```{r}
#| eval: false
parks.sf = retrieved_variables.sf %>%
  filter(variable_name == "parks")
```

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, parks.sf)

rp_3rooms.sf$distance_to_park = as.numeric(
  st_distance(rp_3rooms.sf, 
              parks.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to good primary school

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, shp_malls.sf)

rp_3rooms.sf$distance_to_mall = as.numeric(
  st_distance(rp_3rooms.sf, 
              shp_malls.sf[nearest,],
              by_element=TRUE) )
```

### Proximity to shopping mall

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, shp_malls.sf)

rp_3rooms.sf$distance_to_mall = as.numeric(
  st_distance(rp_3rooms.sf, 
              shp_malls.sf[nearest,], 
              by_element=TRUE) )
```

### Proximity to supermarket

```{r}
#| eval: false
nearest = st_nearest_feature(rp_3rooms.sf, supermarkets.sf)

rp_3rooms.sf$distance_to_supermarkets = as.numeric(
  st_distance(rp_3rooms.sf, 
              supermarkets.sf[nearest,], 
              by_element=TRUE) )
```

## Number of .. within distance variables

```{r}
#| eval: false
buffer_350  = st_buffer(rp_3rooms.sf, 350)
buffer_1000 = st_buffer(rp_3rooms.sf, 1000)
```

### Numbers of kindergartens within 350m

```{r}
#| eval: false
kindergartens.sf = retrieved_variables.sf %>%
  filter(variable_name == "kindergartens")
```

```{r}
#| eval: false
rp_3rooms.sf$nb_of_kindergartens = lengths(
  st_intersects(buffer_350, kindergartens.sf))
```

### Numbers of childcare centres within 350m

```{r}
#| eval: false
child_care.sf = retrieved_variables.sf %>%
  filter(variable_name == "child_care")
```

```{r}
#| eval: false
rp_3rooms.sf$nb_of_childcare = lengths(
  st_intersects(buffer_350, child_care.sf))
```

### Numbers of bus stop within 350m

```{r}
#| eval: false
rp_3rooms.sf$nb_of_bus_stops = lengths(
  st_intersects(buffer_350, bus_stops.sf))
```

### Numbers of primary school within 1km

```{r}
#| eval: false

rp_3rooms.sf$nb_of_primary_schools = lengths(
  st_intersects(buffer_1000, primary_schools.sf))
```

## Saving the data

```{r}
#| eval: false
write_rds(rp_3rooms.sf, "data/geospatial/final_dataset.rds")
```

```{r}
#| eval: false
rp_3rooms.sf = read_rds("data/geospatial/final_dataset.rds")
```

```{r}
#| eval: false
sum(is.na(rp_3rooms.sf))
```

# Hedonic Pricing model

```{r}
#| eval: false
rp_3rooms.sf = rp_3rooms.sf %>%
  rename("01_to_03" = "01 TO 03", "04_to_06" = "04 TO 06", "07_to_09" = "07 TO 09",
         "10_to_12" = "10 TO 12", "13_to_15" = "13 TO 15", "16_to_18" = "16 TO 18",
         "19_to_21" = "19 TO 21", "22_to_24" = "22 TO 24", "25_to_27" = "25 TO 27",
         "28_to_30" = "28 TO 30", "31_to_33" = "31 TO 33", "34_to_36" = "34 TO 36",
         "37_to_39" = "37 TO 39", "40_to_42" = "40 TO 42", "43_to_45" = "43 TO 45",
         "46_to_48" = "46 TO 48", "New_Generation" = "New Generation", 
         "Model_A" = "Model A", "Premium_Apartment" = "Premium Apartment")
```

## Splitting the data

```{r}
#| eval: false
rp_analysis.sf = rp_3rooms.sf %>%
  filter(grepl("202[12]", month))

rp_testing.sf = rp_3rooms.sf %>%
  filter(grepl("2023", month))
```

## Conventional OLS method

```{r}
#| eval: false
colnames(rp_3rooms.sf)
```

```{r}
#| eval: false
rp_analysis.mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease +
                      `01_to_03` + `04_to_06` + `07_to_09` + `10_to_12` + `13_to_15` + 
                      `16_to_18` + `19_to_21` + `22_to_24` + `25_to_27` + `28_to_30` + 
                      `31_to_33` + `34_to_36` + `37_to_39` + `40_to_42` + `43_to_45` + 
                      `New_Generation` + Improved + `Model_A` + Simplified + Standard + 
                      `Premium_Apartment` + DBSS + distance_to_eldercare + 
                      distance_to_food + distance_to_mrt + distance_to_park + 
                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                    data = rp_analysis.sf)

summary(rp_analysis.mlr)
```

## GWR methods

### Converting sf data frame to SpatialPointDataFrame

```{r}
#| eval: false
rp_analysis.sp = as_Spatial(rp_analysis.sf)
```

### Computing adaptive bandwidth

```{r}
#| eval: false
bw_analysis_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + remaining_lease +
                      X01_to_03 + X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + 
                      X16_to_18 + X19_to_21 + X22_to_24 + X25_to_27 + X28_to_30 + 
                      X31_to_33 + X34_to_36 + X37_to_39 + X40_to_42 + X43_to_45 + 
                      New_Generation + Improved + Model_A + Simplified + Standard + 
                      Premium_Apartment + DBSS + distance_to_eldercare + 
                      distance_to_food + distance_to_mrt + distance_to_park + 
                      distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                      nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                      data = rp_analysis.sp, approach="CV", kernel="gaussian",
                      adaptive=TRUE, longlat=FALSE)
```

```{r}
#| eval: false
write_rds(bw_analysis_adaptive, "data/model/bw_analysis_adaptive.rds")
```

### Constructing the adaptive bandwidth gwr model

```{r}
#| eval: false
bw_adaptive <- read_rds("data/model/bw_analysis_adaptive.rds")
```

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + remaining_lease + X01_to_03 +
                            X04_to_06 + X07_to_09 + X10_to_12 + X13_to_15 + X16_to_18 + X19_to_21 +
                            X22_to_24 + X25_to_27 + X28_to_30 + X31_to_33 + X34_to_36 + X37_to_39 + 
                            X40_to_42 + X43_to_45 + New_Generation + Improved + Model_A + Simplified + 
                            Standard + Premium_Apartment + DBSS + distance_to_eldercare + distance_to_food + 
                            distance_to_mrt + distance_to_park + distance_to_mall + distance_to_supermarkets + 
                            nb_of_kindergartens + nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
                          data = rp_analysis.sp, bw=bw_adaptive, kernel = 'gaussian', adaptive=TRUE, 
                          longlat = FALSE)
```

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/model/gwr_analysis_adaptive.rds")
```

```{r}
#| eval: false
gwr_adaptive = read_rds("data/model/gwr_analysis_adaptive.rds")
```

```{r}
#| eval: false
gwr_adaptive
```

## Dropping geometry

```{r}
#| eval: false
coords_train <- st_coordinates(rp_analysis.sf)
```

```{r}
#| eval: false
write_rds(coords_train, "data/model/coords_train.rds" )
```

```{r}
#| eval: false
coords_train = read_rds("data/model/coords_train.rds")
```

```{r}
#| eval: false
rp_analysis.no_geom = rp_analysis.sf %>% 
  st_drop_geometry() %>%
  select(-c(1, 3, 22:23))
```

## Calibrating Random Forest Model

```{r}
#| eval: false
rp_analysis.no_geom = rp_analysis.no_geom %>%
  clean_names()
```

```{r}
#| eval: false
set.seed(2410)
rf <- ranger(resale_price ~ floor_area_sqm + remaining_lease +
               x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + 
               x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + 
               x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + 
               new_generation + improved + model_a + simplified + standard + 
               premium_apartment + dbss + distance_to_eldercare + 
               distance_to_food + distance_to_mrt + distance_to_park + 
               distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
               nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools, 
             data = rp_analysis.no_geom)
```

```{r}
#| eval: false
print(rf)
```

## Calibrating Geographical Random Forest Model

### Calibrating using training data

```{r}
#| eval: false
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + remaining_lease +
                       x01_to_03 + x04_to_06 + x07_to_09 + x10_to_12 + x13_to_15 + 
                       x16_to_18 + x19_to_21 + x22_to_24 + x25_to_27 + x28_to_30 + 
                       x31_to_33 + x34_to_36 + x37_to_39 + x40_to_42 + x43_to_45 + 
                       new_generation + improved + model_a + simplified + standard + 
                       premium_apartment + dbss + distance_to_eldercare + 
                       distance_to_food + distance_to_mrt + distance_to_park +
                       distance_to_mall + distance_to_supermarkets + nb_of_kindergartens +
                       nb_of_childcare + nb_of_bus_stops + nb_of_primary_schools,
                     dframe=rp_analysis.no_geom, bw=bw_adaptive, kernel="adaptive",
                     coords=coords_train)
```

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive250.rds")
```

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

# Testing

```{r}
#| eval: false
tmap_mode("plot")
tm_shape(mpsz.sf)+
  tm_polygons()+
tm_shape(rp_3rooms.sf)+
  tm_dots(col = "blue")+
tm_shape(st_buffer(rp_3rooms.sf, 350))+
  tm_fill(col = "red",
          alpha = 0.1)
```

```{r}
#| eval: false
rp_3rooms.sf %>%
  filter(st_distance(rp_3rooms.sf, kindergartens.sf) <= 350)
```

```{r}
#| eval: false
spatialrisk::concentration(rp_3rooms.sf, kindergartens.sf,
                           value = count, radius = 350)
```
